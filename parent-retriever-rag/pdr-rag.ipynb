{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install langchain openai tiktoken chromadb lark\n",
    "!pip -q install sentence_transformers\n",
    "!pip -q install -U FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "## Text Splitting & Docloader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "encode_kwargs={\"normalize_embeddings\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='What is Xconf?,\"Xconf is an open source, web based, device configuration application for RDK-B devices. It allows to configure parameters such as firmware rules, CPE telemetry and CPE log upload configuration for devices with different MAC address, environment and model.\"\\nExplain Xconf in a few words.,Xconf is a management platform for RDK configuration settings used by engineers and administrators to centrally manage settings (system configuration) on RDK Device.\\nWhat are the different components that make up  Xconf?,\"Xconf has a CPE device component and server-side component.\\n\\nThe Xconf server-side component further consists of three components:\\n\\n- the Xconf Admin Service, including the Xconf Management UI.\\n- the Xconf Data Service part.\\n- the Cassandra NoSQL DB to persist Xconf configurations.\"\\nWhat is the purpose of Xconf Admin Service in the RDK Ecosystem?,\"The Xconf Admin Service handles the request from the Xconf Management UI and modifies the data in Cassandra such as:\\n\\n- Common data sets (MAC list, Model and Environment),\\n- Firmware Configurations; Rule based phased roll out of firmware images\\n- Telemetry profile (markers) and target rules\\n- Log upload configuration (location, upload interval etc.) and target rules\"\\nWhat is the purpose of Xconf Data Service in the RDK Ecosystem?,\"The Xconf Data Service is the component of Xconf that is accessible to CPEs. It provides a public southbound interface to devices. On request of a device, the Xconf Data Service takes configuration data from the Cassandra database and sends the required configuration back to the device.  \\n\"\\nWhere can I find the source code for Xconf?,Xconf Code is restricted and managed by code.rdkcentral.com\\nWhat do the different REST APIs exposed by Xconf Admin Server help me achieve?,\"The XConf Admin Server REST APIs (Xconf Admin Service API Documentation) handle the request from XConf Management UI and modify the data in Cassandra:\\n\\n- Common data sets (MAC list, Model and Environment),\\nCPE target rules\\n- Firmware Configurations; Rule based phased roll out of firmware images\\n- Telemetry profile (markers) and target rules\\n- Log upload configuration (location, upload interval etc.) and target rules\\nREST Controller:\\n\\nThe rest controllers are responsible to create, delete, retrieve and update datasets from UI into Cassandra database.\\n- It is defined under \"\"controller\"\" in XConf-angular-admin.\\n- The controllers handle the request from the Xconf Management UI and modify the data in the Cassandra database.\\n- All the request on the Xconf Data Service are handled by a dispatcher servlet and passed to other servlet as defined in web.xml.\\n- The logging functionality defined in web.xml as a filter logs every request handled by the controllers.\"\\n\"Explain the function of Xconf Data Service in Detail. Explain its sub-components, and basics of what its API does. \",\"The XConf Data Service is the main component of XConf-OSS which communicates with the CPE.\\n\\nThe XConf Dataservice is responsible to fetch the data from the Cassandra database and send it as a response when a CPE makes a request to the XConf Data Service for the system configuration. \\n\\nSub Components:\\n1. Log Management\\n2. Rule Engine: A rule based platform that provides generic data results based on the incoming parameters.\\n\\n3. Data Access\\n4. REST API of the Xconf Data Service\\n\\nThe REST API is responsible to create, delete, retrieve and update datasets into the Cassandra database.\\nAll requests on to the Xconf Data Service are handled by a dispatcher servlet and passed to other servlets as defined in \"\"web.xml\"\" (manifest file of the application).\\nThe logging functionality defined in web.xml as a filter logs every request handled by the controllers.\"\\nWhat is the data persisted by Cassandra for Xconf? Give an example of the data stored in Cassandra.,\"Xconf\\'s data is persisted in the system by using Cassandra DB.\\n\\nExample data that is stored in Cassandra is:\\n1. Common data sets (MAC list, Model and Environment),\\n2. Firmware download configurations,\\n3. CPE log data capture settings and log upload configuration (location, upload interval etc.)\\n4. CPE telemetry marker definition and telemetry profile\\n5 target rules\"\\nHow does Xconf function? How is it related to telemetry and logging?,\"XCONF has client component ie CPE device component and server side component. XCONF\\'s primary purpose is to tell STBs where to send their log files and what telemetry metrics they should be sending. XCONF does not receive logs or telemetry from the boxes. It simply tells the STB where to send log files or telemetry and what specific metrics it should be sending. XCONF also tells STBs when, where (host), and how (protocol) to send this data.\"\\nWhat are the main components that together constitute as Xmidt within the RDK Ecosystem?,\"There are eight main components that together constitute Xmidt. These are:\\n1. WebPA (Tr1d1um)\\n2. Themis\\n3. Petasos\\n4. Talaria\\n5. Caduceus\\n6. Scytale\\n7. Argus\\n8. Consul\"\\nWhat exactly is Xmidt? Explain the purpose of Xmidt in a single line.,XMiDT is a message routing and delivery platform designed to scale to millions of connected clients and billions of messages delivered daily.\\nWhat are some main features of Xmidt?,\"Some main features of xmidt are:\\n- a highly portable Linux client Parodus that provides a simple device integration experience\\n- a cloud infrastructure designed to be highly available\\n- each cloud component is individually scalable providing granular control\\n- the cloud infrastructure is designed to be nearly unlimited in the size\"\\nGive a brief one-liner about the main components that together constitute as Xmidt. Explain each in a single line.,\"Sure! Xmidt is comprised of eight main components. Here is what each of these does:\\n1. Talaria - secure web socket termination server\\n2. Petasos - HTTP redirector for CPE devices\\n3. Scytale - api service front end\\n4. Caduceus - event delivery servicing agent\\n5. Themis -  JWT issuer for authentication and authorization\\n6. Consul - coordination service by HashiCorp\\n7. Argus - Webhook storage and synchronization between Tr1d1um and Caduceus\\n8. WebPA (Tr1d1um) - translation service that provides a RESTful interface for the TR-181 protocol. \"\\nWhat is the business usecase for Xmidt within the RDK Ecosystem? What business processes does Xmidt enable? ,\"Xmidt (pronounced as \"\"transmit\"\") is a set of back end services that provide a communication layer and data path to devices that support the \"\"Web Routing Protocol (WRP)\"\". The Xmidt services are not made available to users. The ingress and southbound interfaces exposed to the internet are for device communication only.\\n\\nXmidt does not implement any business processes, but is an enabler for business processes such as provisioning and de-provisioning and device management. Its primary function and use case is to establish and maintain a secured, bidirectional and stable communication and data path between RDK-B Devices and upstream services.\\n\\n- A device connects to the xmidt cloud via the xmidt client.\\n- Cloud based services can use the xmidt cloud API to send and receive messages from the devices (WebSocket based, bi-directional, asynchronous communication).\\n- Push and pull data from the device.\\n- Secured management of a large number of devices that exist behind firewalls, NATs or other infrastructure.\\n- Highly scalable Device Management for RDK devices.\\n- Provide a highly scalable (linearly), highly available (tolerate 1/3 total loss or greater) mechanism for simultaneously communicating with millions of RDK devices\"\\nWhat are some main services provided by Xmidt? What are some primary functionalities enabled by Xmidt?,\"Xmidt provides the following services:\\n\\n1. Device Management\\n2. GET/SET of TR181 configuration parameters\\n3. Collect ad-hoc device info\\n4. Send notification\\n5. Send analytical device data\\n6. Connectivity: Manage > x million devices\\n7. Mass reconnect of CPE\\n8. Security\\n9. Work efficiently across networks\\n10. Integrate IOT devices with their internet and television experiences seamlessly\\n11. Reports\"\\nHow is Device Management supported in Xmidt? How does Xmidt enable users to manage their routers or devices?,\"Read and update device system configuration from cloud.\\n\\nThe entire time a device is present on our network, we want to be able to communicate with it.\\n\\nUser can configure their gateway from a cloud UI.\\n\\nTurn on or off a feature for X Million customers in a few hours.\"\\nHow does Xmidt enable collection of ad-hoc device info? Why is ad-hoc device data collection needed? What device-related information is captured in logs?,\"Gather CPE telemetry data in real time, or near real time.\\n\\nDevice related information is sent upstream from client on device to server like Firmware Version, XPC configuration data, Model, Serial Number, Boot Time etc. and stored in server statistics / logs\\n\\nNear Real Time Data Gathering of device data for analytic purposes to: ensure service levels; and improve the overall customer \"\"couch experience\"\".\\n\\nDevice related information is sent upstream from client on device to server like Firmware Version, configuration data, Model, Serial Number, Boot Time etc. and stored in server statistics / logs.\"\\nHow does Xmidt enable sending of notifications?,It send upstream events to other cloud services or DPSes for changes on device events using webhooks.\\nWhat is a CPE? Is a CPE same as a router? ,\"CPE stands for \"\"Consumer Premises Equipment\"\" and is a router device presnet in the customer\\'s home.\\n\\nAs defined on Wikipedia:\\n\"\"In telecommunications (... a CPE) is any terminal and associated equipment located at a subscriber\\'s premises and connected with a carrier\\'s telecommunication circuit at the demarcation point \"\"\\nsource: https://en.wikipedia.org/wiki/Customer-premises_equipment\\n\\nThe word CPE is used iterchangeably for a Router or a Device.\\n\\nRouter, CPE &  Device all mean the same thing.\"\\nHow does a Device or CPE establish a session with Xmidt? What is the type of that connection? How is authorization done for this session? How is the websocket session kept alive?,\"The device establishes the secure websockets or WSS connection to RDK Backend or Xmidt Cloud shortly after the device boots.\\n\\nDuring the secure websocket connection is established, the Xmidt Cloud SHALL check the authorization of the device before any user messages are routed to or from the device.\\n\\nIf the Xmidt Cloud is satisfied with the authorization available for the device the Xmidt Cloud SHALL send the message with the status of 200 set.\\n\\nIt SHALL then proceed to route messages to and from the device.\\nThe routing continues until a device is no longer authorized.\\nIf the Xmidt Cloud is not pleased with the certification or authorization provided by the device, it SHALL send the response message with the appropriate status set and MAY disconnect the session.\\n\\nIf a device fails an authorization, the Xmidt Cloud SHALL respond to the device with the authorization response message with the status of 401 set.\\n\\nAt any time, either party may disconnect the session for any reason.\\n\\nThe connection request MAY result in a HTTP redirect to the device, so the device MUST be prepared to support that request.\\n\\nOnce a session is established, websocket ping messages MUST be sent from the device to the Xmidt Cloud periodically. The Xmidt Cloud MUST respond with a websocket pong.\\n\\nAll session initiation originates from the device.\\n\\nThe device SHALL NOT listen for any connection initiated by the Xmidt Cloud.\"\\nWhat is the purpose of the Webhooks stored by Xmidt? Which component in Xmidt ecossytem is responsible for storing these? ,\"\\t\\nRegistered webhooks are the �only stored state�.\\n\\nWebhooks only support registration & auto expire after at most 5 minutes. This simplifies cleanup, migration during upgrades and lets a service that asked for more than it can handle recover.\\n\\nWebhooks are stored by Argus using the Yugabyte DB.\"\\nWhat is WRP? What do you mean by WRP? ,\"WRP stands for Web Routing Protocol and is a core message envelope used by Xmidt and it\\'s associated services.\\n\\n- Mechanism for routing\\n- communication protocols: WebSocket, http, socket\"\\nWhy does Xmidt require WRP?,\"The Xmidt service is designed to be able to travel across different connection types (HTTP, WebSocket, Nanomsg, etc.) and subsequently needs a mechanism to collect all the details needed to deliver, process and respond to a request.\"\\nWhat is Talaria\\'s primary function within the Xmidt family?,\"Talaria\\'s primary function is to interact with millions of devices by maintaining websocket connections, forwarding device events, and sending/receiving requests to/from the devices.\"\\nHow does Talaria communicate with devices?,Talaria communicates with devices securely over a websocket using WRP messages (REE006).\\nHow can Xmidt Cloud clients interact with Talaria devices?,\"Xmidt Cloud clients should not directly interact with Talaria. Instead, they should send requests through Scytale (Xmidt Cloud API).\"\\nWhat are Talaria\\'s device control functionalities? How is Talaria able to manage device connections?,Talaria allows controlling connected devices. It can gate devices (allow/deny connections) and drain devices (remove them from the instance).\\nWhat is Talaria Services\\' main job in the RDK and Xmidt ecosystem?,Talaria Services keeps connections open with devices and securely passes messages between them.\\nDoes Talaria perform any security checks when connecting with routers or CPEs?,\"Yes, Talaria performs a small amount of JWT authorization to verify devices and updates the Consul cluster with device information.\"\\nCan Talaria send events? Can talaria generate events?,\"Yes, Talaria can send events originating from devices and emit its own events, forwarding them to Caduceus.\"\\nWhat are the different Talaria\\'s API endpoints that are available for device interaction?,\"Talaria offers three API endpoints:\\n* /device/{deviceID}/stat - Provides device statistics.\\n* /devices - Retrieves a list of connected devices.\\n* /device/send - Sends WRP messages to a specific device.\"\\nWhat is Petasos? Explain it in a simple sentence.,Petasos is the HTTP redirector component of the Xmidt Cloud. It redirects devices and Scytale requests to the appropriate Talaria node.\\nHow does Petasos redirect devices? How is Petasos able to determine which Talaria instance to redirect devices to?,Petasos uses a consistent hash function based on the device\\'s ID (CM MAC) to determine the target Talaria in the cluster.\\nWhat is the main function of Petasos in the Xmidt and RDK ecosystem? How does Petasos help in managing load?,Petasos redirects HTTP requests to Talaria nodes based on device ID and service discovery configuration. This helps balance load and manage mass reboots.\\nHow does Petasos determine the target Talaria? How is Petasos able to determine the correct destination talaria when a device tried to connect to RDK backend?,\"Petasos communicates with Consul (a service registry) to get the number of active Talaria machines. It then uses the device ID and this information to perform a hash calculation, and selecting the appropriate Talaria in the cluster.\"\\nWhat type of HTTP response does Petasos send? What is the http status code of this request sent by Petasos?,Petasos returns an HTTP 307 redirect response to the target Talaria for both device connections and Scytale requests.\\nHow can Petasos be configured for Talaria coordination? What different configurations are available to set up Petasos for coordinating with a number of Talaria instances?,\"Petasos offers two configuration options:\\n1. Dynamic coordination via Consul (consul option)\\n2. Static configuration with a pre-defined Talaria list (fixed option)\"\\nWhat information does Petasos require in the request header when redirecting devices?,\"Petasos requires the \"\"X-Webpa-Device-Name\"\" header in the request to complete the redirection process.\"\\nWhat is Caduceus and what does it do?,\"Caduceus is an event delivery service agent in Xmidt Cloud. It acts as a publisher-subscriber (pub-sub) system, delivering events to registered webhooks.\"\\nHow does Caduceus handle event delivery? How does Caduceus handle event delivery that is dependent on a variable number of clients?,\"Caduceus focuses on outward-bound event delivery (one-way flow).\\n- It attempts delivery for all events, ensuring messages reach their destination.\\n- It can handle slow-consuming clients and scale to manage a high volume of consumers.\"\\nWhat role does Caduceus play in protecting the Xmidt Cloud? How does Caduceus introduct security in the Xmidt Cloud ecosystem?,\"Caduceus acts as a buffer between Xmidt Cloud services and external consumers requesting events. This protection helps isolate Xmidt services from the demands of consumers, preventing overload.\"\\nWhat is Scytale\\'s role in Xmidt Cloud? Explain Scytale\\'s role in a single line.,\"Scytale acts as the API service front-end, providing a REST API interface for interacting with Xmidt Cloud.\"\\nWhat does Scytale do with incoming requests? How does Scytale treat any incoming request to itself inside the Xmidt Cloud ecosystem?,\"Scytale performs three main tasks for each incoming request:\\n 1. Authenticates the Xmidt Cloud caller. \\n 2. Accepts the request.\\n 3. \"\"Fans out\"\" the request to targeted Talaria machines.\"\\n\"What is \"\"fanning out\"\" a request with respect to Scytale? How is Scytale related to Talaria?\",\"When Scytale receives a request, it doesn\\'t know the exact Talaria handling the targeted device. Instead, it sends the request to all potentially relevant Talaria machines through Petasos, a redirection component.\"\\nHow does Scytale handle requests in a multi-domain deployment?,\"In a high-availability setup with separate Xmidt Cloud domains, Scytale forwards requests to the appropriate Talaria machines within the specific domain where the targeted device resides.\"\\nWhat type of messages does Scytale deliver?,Scytale focuses on delivering API requests to Talaria machines that might be hosting the targeted device connection.\\nDoes Scytale interact directly with devices or routers? Is Scytale able to directly interact with routers or CPEs?,\"No, Scytale doesn\\'t directly interact with devices. It communicates with Talaria machines, which manage device connections.\"\\nWhat does Themis component do? What is the singular responsibility of Themis component within the Xmidt ecosystem?,Xmidt service to issue JWT access tokens for authentication and authorization\\nWhat does Argus do within Xmidt?,Argus is the Xmidt server for storing webhooks to be used by caduceus. It is used for Webhook storage and synchronization between Tr1d1um and Caduceus.\\nHow is data treated by Xmidt and WebPA?,\"Xmidt and WebPA do not persist any user data. \\n\\nData processed is kept in memory,\\n\\nLog data is captured and processed by Prometheus, Filebeat, Logstash and Elasticsearch.\\n\\nAggregated log data is persisted by Elasticsearch.\\n\\nLog Data is visualized by Grafana (Prometheus and Elasticsearch as data sources).\"\\nGive me some trivia and fun facts about Xmidt.,Xmidt cloud and associated components are named after Greek gods or items of the Greek gods\\nTell me a fun fact about Parodus. Is Parodus a Greek word?,\"Parodus is actually an ancient greek word for \"\"entrance\"\".\"\\nTell me a Fun fact about Talaria. Is it related to any Greek God in any way?,\"Talaria were actually the winged sandals, a symbol of the Greek messenger god Hermes. It is pronounced as \"\"tuh-laa-ree-uh\"\".\"\\nGive a trivia or a fun fact about Scytale. How do I pronounce the word Scytale? ,\"Scytale is actually the stick for transposition cipher. In Ancient Greek, it means a \\'baton\\' or a \\'cylinder\\'. It is pronounced as \"\"skit-uh-lee\"\".\"\\n\"How does one pronounce \"\"Petasos\"\"? Also, tell me something interesting about Petasos.\",\"Petasos in Greek legends was actually the funky hat worn by the Greek messenger god Hermes, and is pronounced as \"\"pet-uh-sos\"\".\"\\nWhat is the origin of the word Caduceus? Give me a trivia about the word along with its pronounciation. ,\"Caduceus in Greek legends was the name for Hermes� wand. It is said the wand would wake the sleeping and send the awake to sleep. It was symbol of commerce and negotiation. Pronounced as \"\"kuh-doo-see-us\"\".\"\\n\"Who was Themis? How is the Themis component in the Xmidt ecossytem related to Greek mythology? Tell me something interesting about Themis,\",\"Themis was an ancient Greek Titaness. She is described as deity of good counsel, order and righteousness.\"\\nWhat Signing algorithm does Themis use for its Tokens?,Themis used the alogorithm RSA-256 for signing. RSA-256 is the required JWT signing algorithm to use by Themis.\\nWhat is the token.key.file config in Themis Configuration? Can I auto-generate a key within Themis?,\"Token.key.file is the system path to a file where the key is stored. If set, this file must exist and contain either a secret or a PEM-encoded key pair. If this field is not set, a key is generated.\"\\nHow is Northbound Access managed by Themis? What is Themis\\' role in managing Northbound access?,\"Northbound access is used by Caduceus, WebPA (Tr1d1um) and Argus:\\n\\n1. Caduceus and tr1d1um will request a token from Themis\\n2. Argus will get the keys from Themis to set the adminMode, to enforce so that it gets all webhooks everytime.\"\\nHow is Southbound Access managed by Themis?  What is Themis\\' role in managing Southbound access?,\"Southbound access token (JWT) is requested by a CPE or Router\\n\\n- a CPE calls this Themis instance and is relayed to Xmidt listener ? Device Manager ? Resource Inventory\\n- remote configuration call of Themis to the Xmidt Listener.\"\\nWhat data formats does the XMIDT Cloud API support?,\"The XMIDT Cloud API supports three data formats: application/octet-stream, application/msgpack, and application/json.\"\\nWhat is the request format for the XMIDT Cloud API?,\"You need to send a POST request with a JSON, MsgPack, or HTTP header encoded WRP message specifying the destination in the WRP.\"\\nWhat format is the API response returned in inside the RDK or Xmidt ecosystem?,The response is returned in the same format (schema) as the request you sent.\\nHow does Scytale connect to Talaria for API calls? How is Petasos related to Scytale and Talaria?,\"Scytale communicates with Talaria through Petasos, a redirection component.\"\\nWhat does the GET  /device/{deviceID}/stat  API do in the case of Scytale?,\"The /device/{deviceID}/stat API is used to get Device Statistics. The HTTP Method call is of type GET.\\n\\n- Returns the statistics of the connected device, including information such as uptime and bytes sent.\\n- This information is retrieved from the Talaria that the device is connected to.\\n- deviceID should be like  mac:112233445566\\n- Note: If the Device is not connected then 404 is returned.\"\\nWhat does the POST /device API Call do in the case of Scytale? What happens when the device is not connected while running this call?,\"Mainly, the /device API is of POST type, and is used to Send WRP message to Device or CPE.\\n\\n- Send Web routing protocol (WRP) message to Device\\n- Accepts a WRP message encoded in a valid WRP representation\\n(generally msgpack or json)\\n- Forwards the request to the correct Talaria.\\n- If the Device is not connected then 404 is returned.\"\\nWhat is the metrics API in Scytale? How can I access it?,The Metrics API Call for the Scytale Microservice is exposed at the /metrics endpoint. It is accessible with a HTTP Call type of GET.\\nWhat is the health API in Scytale? How can I access it?,The GET /health API is to fetch the current health status of Scytale component.\\nWhat is the fanout concurrency limit in Scytale? ,The current fanout concurrency limit in Scytale is 1000. Only a 1000 number of calls can be sent to all Talaria in parallel or concurrently.\\nHow many redirects does Scytale allow for each fanout.,Scytale actually defealts to unlimited redirects for each fanout. This value is configurable in Scytale configuration. \\nWhere is the configuration code for Scytale available? Is this code public?,The config code for Scytale is opensource and available on Github at: github.com/xmidt-org/scytale/blob/master/scytale.yaml\\nWhat are some metrics published by Scytale?,\"Scytale actually publishes a ton of useful metrics to understand what happeneing with the service at any given point. Some important metrics published for Scytale are:\\n\\n1. auth_validation: Counter of Auth validation success/failed.\\n2. sd_instance_count: Service discovery instance count.\\n3. sd_error_count: Consul service discovery error count\\n4. sd_update_count: The total count of updates from the service discovery backend for a particular service\\n5. sd_last_error_timestamp: At what time last error occurred.\\n6. sd_last_update_timestamp: At what time the last service is updated.\\n7. received_wrp_message_total: Total no of WRP Messages received� in scytale\\n8. api_requests_total(code 2xx to 3xx): no of requests received on a service\\n9. api_requests_total(code 4xx to 5xx): No of failed requests\\n10. in_flight_requests: No of the current request which is under processing\\n11. active_connections: The number of active connections associated with� the server\\n12. rejected_connections: The total number of connections rejected due to exceeding the limit\\n13. request_duration_seconds: latencies for requests\\n14. request_size_bytes: request sizes for requests.\\n15. response_size_bytes: response sizes for requests.\\n16. time_writing_header_seconds: latencies for writing HTTP headers.\\n17. maximum_processors: number of current maximum processors this processes is allowed to use\"\\nHow do you get list of connected devices in Talaria? Show me how to use it. Is it recommended to run this in all environments?,\"We can get a list of connected devices in Talaria using the /api/v2/devices API.\\n\\nIt can be triggered with a curl as:\\ncurl -i -H \"\"Authorization: Basic AUTHOKEN\"\" HOSTNAME:PRIMARY_PORT/api/v2/devices\\n\\nWhere HOSTNAME is your DNS record, docker container, or ip address listening on the PRIMARY_PORT. And AUTHOKEN is the inbound.authKey\"\" in the yaml configuration file.\\n\\nNote that this is a very expensive command. Do NOT run it in production environment.\"\\nGive a system overview of Xconf. What are its constituent applications?,\"Xconf is an open source device configuration application. It is a web / API call based application, where users can configure different firmware rules, templates and configuration for different CPEs  with different MAC address, environment and model. It is a platform for technicians to define different MAC list, Environments and Models to associate them to different Firmware Rules and Configuration.\\n\\n Xconf OSS is Primary composed of two different applications:\\n1. Xconf Management UI\\n2. Xconf Data Service\"\\nHow does Xconf help with Firmware upgrades? What info does Xconf provide to CPEs relating to router firmwares.,\"XConf provides option to provide these below details for the CPEs for firmware upgrade.\\n\\n1. Which firmware version to download\\n2. From where to download\\n3. How (protocol) to download\"\\nWhat databses does Argus support for persistence of data?,\"Argus supports the following ways for data-persistence:\\n\\n- in memory\\n- yugabyte DB\\n- AWS dynamo DB\"\\nHow does authorization work in Argus?,\"The Themis configuration can specify the authorization to access the (northbound) Argus API.\\nIt is used to allow or disallow if a given request to Argus is allowed to execute a webhook registration.\"\\nWhat does the \\'accessLevel\\' config do in Argus\\' configuration YAML?,accessLevel defines config around the injection of an attribute to bascule tokens (JWT access token issued by Themis) which application code can leverage to decide if a given request is allowed to execute some operation.\\nWhat does the \\'capabilityCheck\\' config do in Argus\\' configuration YAML? Can I control how a request is handled with this config?,\"CapabilityCheck provides the details needed for checking an incoming JWT\\'s capabilities.\\n\\nIf the type of check isn\\'t provided, no checking is done.\\n\\nThe type can be \"\"monitor\"\" or \"\"enforce\"\".\\n\\n- If \"\"monitor\"\" is provided, the capabilities are checked but the request isn\\'t rejected when there isn\\'t a valid capability for the request. Instead, a message is logged.\\n- When \"\"enforce\"\" is provided, a request that doesn\\'t have the needed capability is rejected.\"\\nHow is \\'accessLevel\\' config different from \\'capabilityCheck\\' config in Argus\\'s configuration file? What is the difference between capabilityCheck and accessLevel in Argus component\\'s configuration?,\"accessLevel differs from capabilityCheck:\\n\\n- capabilityCheck verifies whether a user is allowed to use an API endpoint.\\n- accessLevel allows more complex access hierarchy.\\n- accessLevel assigns a number to the user\\'s request which application code can use for security purposes.\\n- An access level is defined as a non-negative number and the higher the number, the higher the access the request has for the target application.\"\\nHow is a capability defined in Argus configuration? What guidelines do I need to follow when defining a capability. ,\"Capability format:\\n\\n- The capability is expected to have the format: {prefix}{endpoint}:{method}\\n- The prefix can be a regular expression.\\n- If it\\'s empty, no capability check is done.\\n- The endpoint is a regular expression that should match the endpoint the request was sent to.\\n- The method is usually the method of the request, such as GET.\\n- The acceptAllMethod: \"\"all\"\"  method is a catchall string that indicates the capability is approved for all methods.\"\\nWhat is YugabyteDB?,\"YugabyteDB is a high-performance transactional distributed SQL database for cloud-native applications, developed by Yugabyte\"\\nExplain to me the benefits of YugabyteDB. In what scenarios is using YugabyteDB useful?,\"YugabyteDB is a high-performance, cloud-native, distributed SQL database that aims to support all PostgreSQL features. It is best suited for cloud-native OLTP (i.e., real-time, business-critical) applications that need absolute data correctness and require at least one of the following: scalability, high tolerance to failures, or globally-distributed deployments.\"\\nWhy use YugabyteDB for Argus? Why is Yugabyte a top choice for using as a DB in Argus?,\"YugabyteDB is a high-performance,  cloud-native, distributedSQL database that aims to support all PostgreSQL features. It is used by Argus to persist the incoming WRP messages.\"\\n\"How is Yugabyte set-up and deployed for Argus? Provide me the setup configuration details for deploying Argus as a Kubernetes workload, and also mention the cpu and memory required to set this up.\",\"By default Argus is deployed with an in-memory database.\\n\\nHowever Argus can also be backed by a yugabyteDB.\\n\\nOn EKS Yugabyte can work only in cluster mode.\\n\\nFor a multi-AZ deployment, it is recommended to set it up with a 2 replicas.\\nIn every replica, there would be 2 pods of a master and a tserver.\\n\\nResources used would be:\\n-  500m vCPU for each pod\\n- hence, totaling to 2 dedicated vCPUs\\n- 1 GB RAM for each Masters\\n- 2 GB RAM for each tservers.\"\\nHow do I define a table in Yugabyte for Argus or Xmidt? What ycqlsh command do I need to run to create a Keyspace in Yugabyte?,\"Open the ycqlsh command. Inside that, enter:\\n\\nCREATE KEYSPACE IF NOT EXISTS argus;\\nCREATE TABLE argus.gifnoc (\\n    bucket VARCHAR,\\n    id VARCHAR,\\n    data blob,\\n    PRIMARY KEY (bucket, id))\\n    WITH default_time_to_live = 300\\n    AND transactions = {\\'enabled\\': \\'false\\'};\"\\n\"What is the max TTL or lifetime of a messgae stored in YugabyteDB for Xmidt? Is there a default value, and in what scenario should I customize this time?\",\"The max TTL of any data item stored in Yugabyte is configured in Argus\\' configuration file. \\n\\nThe property \\'itemMaxTTL\\' is responsible for defining the duration after which items can expire in Yugabyte.\\n\\nThe default value of itemMaxTTL is 24 hours, and can be customized for different data-privacy scenarios.\"\\nWhat different APIs are available in Argus to manage data? What is it\\'s basepath? Also explain the different http types of each API along with its purpose.,\"Argus has 4 main APIs for managing data. The base path of the API is: <FQDN>:<SERVER_PORT>/api/v1\\n\\nThe different APIs are:\\n1. PUT /store/{bucket}/{id} : this is of PUT type, and is used to persist data in  the configured database.\\n2. GET /store/{bucket}/{id}: this is of GET type, and is used to return  unique record identified by bucket and id.\\n3. GET /store/{bucket}: this is of GET type, and is used to return all the Records for the bucket.\\n4. DELETE /store/{bucket}/{id}: this is of DELETE type, and is used to delete a record if exists in configured DB.\"\\nWhat APIs are available for Argus to check its metrics and health?,\"There are 2 APIs available in Argus for checking its metrics and health:\\n\\n1. GET /metrics: fetches different metrics available for Argus \\n2. GET  /health: fetches current microservice health for Argus\"\\nWhat are some important metrics published by Themis? What critical metrics related to Themis\\' server and client status can I track the count of different request types?,\"Themis publishes some important metrics that describe the state of the microservice at that point. These are:\\n\\n1. server_request_count: How Many requests are served by themis\\n2. server_request_duration_ms: Duration stat of themis issue request\\n3. server_requests_in_flight: Currently active� issue requests\\n4. client_request_count: Total no of remote server calls count\\n5. client_requests_in_flight: Currently active remote server calls\"\\nWhat are some configurations that I can tweak within the config file of Argus when using DynamoDB as backend database for Argus?,\"Following are some important configurables that one can use to customize Argus\\' dependency on DynamoDB: \\n\\n1. dynamo.endpoint: is used to set a custom aws endpoint. (Default Value: None)\\n2. dynamo.table: is the name of the table that is already configured with bucket and id as the key. (Default Value: None)\\n3. dynamo.region: is where request should go to. (Default Value: None)\\n4. dynamo.maxRetries: is the maximum times the application will retry the request to the db. (Default Value: 3)\\n5. dynamo.accessKey: is the AWS accessKey to access dynamodb. (Default Value: None)\\n6. dynamo.secretKey: is the AWS secretKey to go with the accessKey to access dynamodb. (Default Value: None)\\n7. itemTTL.defaultTTL: is used if not ttl is provided via the api. (Default Value: 5m)\\n8. itemTTL.maxTTL: maxTTL is limit the maxTTL provided via the api. (Default Value: 1y)\"\\nWhat are some configurations that I can tweak Argus when using YugabyteDB as backend database?,\"Following are some important configurables that one can use to customize Argus\\' dependency on Yugabyte: \\n\\n1. yugabyte.hosts: is and array of address and port used to connect to the cluster (Default Value: [None])\\n2. yugabyte.database: is the name of the database being connected to. (Default Value: None)\\n3. yugabyte.opTimeout: is the timeout for database calls after argus is connected. (Default Value: None)\\n4. yugabyte.username: is the username to use when connecting to the database. (Default Value: None)\\n5. yugabyte.password: is the password to use when connecting to the database. (Default Value: None)\\n6. yugabyte.sslRootCert: is the root cert to use when connecting to the database. The SSLKey and SSLCert must also be provided in order to connect securely. (Default Value: None)\\n7. yugabyte.sslKey: is the SSL key to use when connecting to the database. The SSLRootCert and SSLCert must also be provided in order to connect securely. (Default Value: None)\\n8. yugabyte.sslCert: is the SSL cert to use when connecting to the database. The SSLKey and SSLRootCert must also be provided in order to connect securely. (Default Value: None)\\n9. yugabyte.enableHostVerification: If you want to verify the hostname and server cert (like a wildcard for cass cluster) then you should turn this on This option is basically the inverse of InSecureSkipVerify (Default Value: FALSE)\"\\nWhat are some important metrics published by Argus? I would like to scrape these exposed metrics using Promtheus. ,\"Following are some important metrics that one are exposed by Argus to report its current system state:\\n\\n1. server_request_count: How Many requests are served by Argus.\\n2. server_request_duration_ms: tracks incoming request durations in ms.\\n3. server_requests_in_flight: tracks the current number of incoming requests being processed\\n4. db_queries_total: The total number of DB queries Argus has performed\\n5. db_query_duration_seconds: A histogram of latencies for queries\\n6. auth_capability_check: How many times authorization check is executed.\\n7. auth_validation: How many auth was successful/failed.\"\\nHow are events delivered to registered webhooks? Which component of the Xmidt ecosystem is responsible for achieving this?,Caduceus is the Xmidt service for delivering events to registered web hooks.\\nWhat is the core function of Caduceus? Explain this in a single-line. ,\"Caduceus is an event delivery service agent in Xmidt Cloud, acting as a pub-sub system for event delivery to registered consumers.\"\\nWhat type of event flow does Caduceus handle? Is this flow unidirectional or bi-directional?,\"Caduceus focuses on outward-bound event delivery, meaning events move in one direction (from Xmidt Cloud to consumers).\"\\nHow does Caduceus ensure reliable event delivery?,\"Caduceus attempts delivery for all events, ensuring messages reach their destination.\"\\nHow does Caduceus handle consumer scalability?,Caduceus can efficiently manage both slow-consuming clients and a high volume of consumers.\\nDoes Caduceus offer any protective measures for Xmidt Cloud?,\"Yes, Caduceus acts as a buffer, protecting Xmidt Cloud services from the demands of external consumers requesting events, preventing overload.\"\\nWhat role does Caduceus play in message direction?,\"Caduceus operates as a one-way messaging system, delivering events outward from Xmidt Cloud.\"\\nWhere is the codebase for Caduceus hosted?,The cidebase for Caduceus is hosted on Github. https://github.com/xmidt-org/caduceus\\nWhere was Caduceus created? Who created Caduceus?,\"The Caduceus component was created by Comcast, and then later made part of xmidt-org.\"\\nWhat are the different APIs available within Caduceus for Xmidt events?,\"Caduceus has one main function, which is to deliver events to a consumer. Caduceus has 3 main endpoints or APIs for handling xmidt events: \\n\\n1. POST  api/v4/notify : To receive events\\n2. POST /hook : To register a webhook\\n3. GET /hooks  : To get webhooks\"\\nDescrive the api/v4/notify API in Caduceus.,\"The /api/v4/notify API is of http type POST and is used in Caduceus to:\\n1. Accept a msgpack encoding of a WRP Message.\\n2. If a webhook is registered and matches the device regex and event regex, the event will be sent to the webhooks registered url.\"\\nDescribe the /hook API in Caduceus.,\"The /hook API is of http-type POST and exists in Caduceus to:\\n1. Register a webhook and get events\\n2. The consumer must send an http POST request to caduceus that includes the http url for receiving the events and a list of regex filters.\"\\nDescrive the /hooks API in Caduceus.,\"The /hooks API is of http type GET and is used in Caduceus to:\\n1. return all the webhooks and their configuration.\\n2. Speed up caduceus start up time\\n3. Test the registration of webhooks\"\\nWhat APIs are available for Caduceus to check its metrics and health?,\"There are 2 APIs available in Caduceus for checking its metrics and health:\\n\\n1. GET /metrics: fetches different metrics available for Caduceus \\n2. GET  /health: fetches current microservice health for Caduceus\"\\nWhat is the significance of the authHeader configuration in Caduceus?,\"While the default value of authHeader is none in the cade of Caduceus, it can be configured with a list of basic auth headers that caduceus will accept as authorization\"\\nWhat are some important configuration values that I can tweak in Caduceus\\' config file that are related to event delivery?,\"Caduceus has a lot of configuration values that can be customised or tweaked to influence how events are delivered. Some of these configurations along with their default values are:\\n\\n1. sender.numWorkersPerSender: defines the maximum number of outgoing concurrent HTTP client requests to a particular webhook url. (Default Value: None)\\n2. sender.queueSizePerSender: the maximum queue depth (in events) the sender will store before cutting off the webhook because the delivery pipeline has backed up. (Default Value: None)\\n\"\"3. sender.cutOffPeriod: is the duration of time the webhook will be cut off if the delivery pipeline gets backed up.\\nAll outstanding events will be dropped, as well as all-new events otherwise destined for this webhook will be dropped.\\nThis period of time is to allow the webhook server time to recover. (Default Value: None)\"\"\\n4. sender.linger: is the duration of time after a webhook has not been registered before the delivery pipeline is torn down. (Default Value: None)\\n\"\"5. sender.disableClientHostnameValidation: provides a way to bypass TLS validation failures on HTTPS requests when sending events to webhooks. \\nSetting this to true allows for a potential man-in-the-middle scenario between a caduceus and a webhook. (Default Value: FALSE)\"\"\\n6. sender.deliveryRetries: is the maximum number of delivery attempts caduceus will make before dropping an event (Default Value: None)\\n7. sender.deliveryInterval: is the time to wait after a failed delivery attempt before attempting to deliver again (Default Value: None)\\n8. sender.responseHeaderTimeout: is the time to wait for a response before giving up and marking the delivery a failure (Default Value: None)\\n9. sender.retryCodes: provides a list of http status codes for caduceus to match against. If the response code given by a webhook matches a code in this list, Caduceus will try to send the event again. (Default Value: None)\"\\nWhat are some important configuration values that I can tweak in Argus\\' config file that are related to how Argus stores Webhooks?,\"Following are some important configurables in Argus\\' config file when it comes to storing webhooks:\\n\\n1. webhook.argus.bucket: to store and retrieve webhooks. (Default Value: None)\\n2. webhook.argus.address: it is the location to talk to argus. (Default Value: None)\\n3. webhook.argus.pullInterval: is how often to call argus to update the webhook structure. (Default Value: None)\\n4. webhook.argus.auth.basic: Basic/static Auth (Default Value: None)\\n5. webhook.argus.auth.JWT.requestHeaders: are added to the request for the token. (Default Value: None)\\n6. webhook.argus.auth.JWT.authURL: is the URL to access for the token (Default Value: None)\\n7. webhook.argus.auth.JWT.timeout: is how long the request to get the token will take before the timeout (Default Value: None)\\n8. webhook.JWTParserType: applicable values[simple,raw] (Default Value: simple)\\n8. webhook.argus.auth.JWT.buffer: is the length of time before a token expires to get a new token (Default Value: None)\"\\n\"What are some important metrics published by Caduceus? What metrics exposed by caduceus can I track in Prometheus to get an idea about the volume of workers, volume of events, requests handled, size of those requests for Caduceus in the Xmidt ecosystem in RDK?\",\"Some important metrics that are published by Caduceus give a glimpse into the state of Caduceus at any given point. It gives an idea about the volume of workers, volume of events, requests handled, size of those requests etc. \\n\\nIn detail, these metrics are:\\n1. webhook_list_size_value: The caduceus is not able to get the webhooks from argus\\n2. chrysom_polls_total (outcome=\"\"success\"\"): The caduceus is not able to poll the webhooks from argus\\n3. chrysom_polls_total (outcome=\"\"failure\"\"): The caduceus is not able to poll the webhooks from argus\\n4. error_request_body_count: Invalid request recevied in caduceus\\n5. empty_request_body_count: Empty request recevied in caduceus\\n6. drops_due_to_invalid_payload: The caduceus is not able to unmarshal the WRP\\n7. delivery_count(code=2XX): Caduceus hasn\\'t got any success in delivering the message\\n8. delivery_count(code=non 2XX): Caduceus hasn\\'t got any success in delivering the message\\n\"\"9. slow_consumer_dropped_message_count(url=$webhookUrl,reason=queue_full): Caduceus Queue is full, now it will be dropping all the messages.\\nTo fix for this\\n1. increase the event receviers\\n2. increase the queue size in caduceus.\"\"\\n10. slow_consumer_dropped_message_count(url=$webhookUrl,reason=cut_off): Caduceus will stop sending the messages.\\n11. slow_consumer_dropped_message_count(url=$webhookUrl,reason=inavalid_config): Webhook endpoint URL is wrong\\n12. slow_consumer_dropped_message_count(url=$webhookUrl,reason=network_err): Caduceus is not able to send the events to webhook endpoint\\n13. drops_due_to_panic: Error while sending a WRP message to the webhook endpoint\\n14. slow_consumer_cut_off_count: How many time cut off is� happened for a webhook\\n15. outgoing_queue_depths: How many events are there in queue\\n16. incoming_event_type_count: How many events are received by the caduceus\\n17. consumer_renewal_time: At what time webhook is refreshed\\n18. consumer_deliver_until: Till what time this webhook will be delivering the WRP messages\\n19. consumer_drop_until: Till when Caduceus will be dropping the messages.\\n20. consumer_delivery_workers: How many workers are currently delivering the messages\\n21. consumer_delivery_workers_max: Maximum number of worker for a webhook\\n22. api_requests_total(code 2xx to 3xx): no of requests received on a service\\n23. api_requests_total(code 4xx to 5xx): No of failed requests\\n24. in_flight_requests: No of the current request which is under processing\\n25. active_connections: The number of active connections associated with� the server\\n26. rejected_connections: The total number of connections rejected due to exceeding the limit\\n27. request_duration_seconds: latencies for requests\\n28. request_size_bytes: request sizes for requests.\\n29. response_size_bytes: response sizes for requests.\\n30. time_writing_header_seconds: latencies for writing HTTP headers.\\n31. maximum_processors: number of current maximum processors this processes is allowed to use\"\\nWhat could be a reason if Caduceus is not receving any events? How should I fix this issue?,\"If Caduceus is not receving any events, then it means that Talaria has not been configured correctly to talk to Caduceus. \\n\\nTo fix this, update the Talaria configuration with the right path of Caduceus service.\"\\nWhat are some Xmidt services that support tracing?,\"Support for tracing via Opentelemetry can be enabled for the following Xmidt components through their respective configs:\\n\\n- Tr1d1um\\n- Scytale\\n- Argus\\n- Caduceus\\n- Talaria\\n- Petasos\\n- Themis\"\\nWhat is the Parodus component in Xmidt? Where is the Parodus component siutated?,Parodus is a websocket client which connects to server in the cloud & delivers messages to and from services running on the gateway device which is the CPE or the Router. Parodus is a platform independent websocket client and nanomsg server that lives on the CPE or the Client Premises Equipment.\\nExplain the function of Parodus in Xmidt Architecture. Give the answer in 3 or 4 lines and in a paragraph. ,\"Parodus is the client device broker. It is tasked with connecting up to the specified fabric.xmidt.example.com URL so that it can be managed as well as provide near real time upstream data. Parodus also has hooks to provide for acquiring JWT credentials to present to Talaria upon connection. After connection, it\\'s role is to maintain connection to the XMiDT cluster at all times as well as shuttle messages between a client and the cluster.\"\\nExplain the function of Talaria in Xmidt Architecture. Explain this briefly in just 3 or 4 lines.,\"At the other end of the Parodus connections is a collection of Talaria machines. Their job is to keep the connections open to the devices and pass messages to and from the device. A small amount of JWT authorization checking is done in Talaria to ensure the device is allowed, as well as keeping the Consul cluster up to date with it\\'s existence.\"\\nExplain the function of Petasos in Xmidt Architecture. What does Petasos do? Explain Petasos\\' functioning in 3-4 lines.,\"Petasos machines are a redirector service based on a consistent hash. The Petasos machines for a data center/region/zone communicate with Consul to determine the number of Talaria machines. Once it knows the count of Talaria instances, it can perform a hash based on the device-id of the incoming device to determine where in the cluster the device should go. An HTTP redirection response is sent to the device which then connects directly to the target Talaria.\"\\nExplain the function of Caduceus in Xmidt Architecture. How does Caduceus help in event delivery?,\"For outward bound events (flow only one direction), Caduceus provides the needed protection of the XMiDT cluster from the demands of the servers in other services that ultimate wish to consume the events. Delivery is attempted for all events & Caduceus handles both slowly consuming clients as well as scaling to very large clients.\"\\nExplain the function of Scytale in Xmidt Architecture. How does Scytale function in a cloud environment that is spread across multiple AZs or Availability Zones?,\"The XMiDT API is served by Scytale. The basic job of this machine is to authenticate the cloud caller, accept the request and fan out the request to all possible Talaria machines that are targeted by the request. If an XMiDT cluster is deployed in 3 separate data centers or availability zones for high availability, Scytale will be configured (either directly or through Consul) to forward a request to a particular device to the 3 possible Talaria machines where the device could reside.\"\\nWhat is WebPA with respect to Xmidt or RDK? What is the significance of WebPA in the Xmidt or RDK Ecosystem?,WebPA is the communication channel from Cloud to RDK based home gateway devices. It helps to manage devices from Cloud. WEBPA protocol provides functionality of read/write access to device management parameters in an efficient manner as compared to TR-69 or SNMP.\\nExplain WebPA as briefly as possible.,\"WebPA is a \"\"Protocol Agent\"\" that facilitates remote management of gateway devices.\"\\nWhy does XMiDT need WRP? ,\"The XMiDT service is designed to be able to travel across different connection types (HTTP, Websocket, Nanomsg, etc) and subsequently needs a mechanism to collect all the details needed to deliver, process and respond to a request.\"\\nWhat happens when Parodus boots up on a Router?,\"Parodus first uses CM MAC to identify the device, then connects to the fabric URL, and finally redirects to a Talaria server.\"\\nDoes Parodus maintain a persistent connection to Xmidt or RDK Backend? Does it also have a reconnection logic?,\"Yes, Parodus utilizes a persistent WebSocket (WSS) connection to ensure reliable communication with Xmidt Cloud. This WSS connection allows for real-time data exchange. If the connection is lost for any reason, Parodus automatically attempts to re-establish it.\"\\nWhat is Libparodus used for?,WebPA client (parodus2ccsp) and other micro-services use Libparodus to communicate with the Parodus client.\\nWhat role does Nanomsg play?,It\\'s an open-source library for internal socket abstraction between Parodus and other components.\\nHow does Parodus handle data flow?,It sends downstream cloud requests to the device\\'s corresponding service and sends upstream events to the cloud.\\nWhat is the primary function of Parodus as an Xmidt client?,\"It provides communication between clients and the server, acting as a client device broker/coordinator.\"\\nAre there different Parodus client types available in the Xmidt ecosystem?,\"Yes, Parodus offers two client types to cater to different needs: \\n- Send & Receive Clients: These clients have full functionality, allowing them to both send requests to Xmidt Cloud and receive data from the cloud. \\n- Receive Only Clients: These clients can only receive data (upstream events) from Xmidt Cloud, but cannot send requests themselves.\"\\nHow Parodus does communicate with other clients on the same router or device?,\"\\n- Nanomsg is used to communicate with Parodus on The device.\\n- Nanomsg acts like internal bus.\\n- Client Server model is used where Parodus is server.\\n- This replaces The previously defined RPC mechanisms.\\n- Message queues are implemented for both incoming WRP messages and outgoing WRP messages from nanomsg.\\n-  This is same as nopoll incoming and outgoing queue architecture\"\\nBriefly explain what Petasos is. Try it in as short as possible.,HTTP redirector component of the Xmidt Cloud\\nWhat\\'s Petasos\\' primary function? How is a router or device\\'s MAC Address important to Petasos?,\"Petasos acts as an HTTP redirector, playing a crucial role in directing incoming requests to the appropriate Talaria node within the Xmidt Cloud infrastructure. It achieves this by leveraging a consistent hashing function based on the device\\'s unique MAC address.\"\\nWhat hashing mechanism does Petasos use? Are there any benefits of using a consistent-hashing function?,\"Petasos employs a consistent hashing function to distribute device connections across Talaria machines. This hashing function ensures that devices with similar IDs are likely to be directed to the same Talaria node, promoting load balancing and efficient management.\"\\nHow does Petasos protect Talaria during mass reboots? What is the role of Petasos in the scenario when a large number of routers reboot?,\"During scenarios like mass reboots where a large number of devices attempt to connect simultaneously, Petasos plays a vital role in managing the load on Talaria machines. By calculating the target Talaria for each device based on its ID, Petasos distributes the incoming connection requests across different Talaria nodes. This helps prevent overloading any single Talaria machine and ensures smoother handling of mass reboots.\"\\nHow does Petasos determine the number of Talaria machines? How is Petasos made aware of the number of Talaria instances available for it?,\"Petasos relies on Consul, a service registry tool, to discover the available Talaria machines within a specific domain (data center/region/zone). By communicating with Consul, Petasos obtains an up-to-date count of active Talaria machines, enabling it to make informed decisions about redirecting device connections.\"\\nWhat information does Petasos use to determine the target Talaria? What pieces of information are required to determine the correct destination Talaria?,\"Petasos utilizes two key pieces of information to identify the appropriate Talaria for an incoming device connection:\\n- Device ID: The device\\'s unique MAC address serves as the primary factor. Petasos applies a consistent hashing function to this ID, resulting in a specific Talaria node responsible for handling that particular device.\\n� Service Discovery Configuration: In conjunction with the device ID, Petasos also considers service discovery information to ensure the redirection aligns with the overall service configuration within Xmidt Cloud.\"\\nDoes Petasos offer configuration options for Talaria coordination? What confuguration options exist for managing Talaria coordination?,\"Yes, Petasos provides flexibility in managing Talaria coordination through two configuration options: \\n- Dynamic coordination via Consul (consul option): This approach leverages Consul\\'s service registry to dynamically discover and adapt to changes in the number of available Talaria machines. \\n- Static configuration with a predefined Talaria list (fixed option): Here, an administrator can define a static list of Talaria nodes that Petasos will use for redirection. This option might be suitable for scenarios with a fixed Talaria deployment.\"\\nHow does Petasos handle URI paths in requests? Does it fo any validation of the URI path for the requests it receives?,\"Petasos primarily focuses on directing the device or service towards the correct Talaria node. When a request arrives, Petasos takes the entire request, including any URI paths (e.g., /api/v2/device, /api/v2/device/send), and redirects it to the target Talaria. Petasos itself doesn\\'t perform any parsing or validation of the URI path; it simply forwards the complete request for the Talaria to handle.\"\\nWhat HTTP header is required by Petasos for request completion? What is the significance of the X-Webpa-Device-Name Header in requests received by petasos?,\"To successfully process a request and perform redirection, Petasos mandates the inclusion of the \"\"X-Webpa-Device-Name\"\" header in the request. This header likely plays a role in identifying the specific device or service making the request and ensures proper handling within Petasos.\"\\nHow does the Petasos or Xmidt Device API handle HTTP redirections?,\"The Xmidt Device API leverages HTTP redirection mechanisms for specific situations. When interacting with the Xmidt Device API, clients should be prepared to encounter and follow HTTP redirections. These redirections are not indicative of errors but rather a normal part of the communication flow within Xmidt Cloud.\"\\nWhat is the maximum redirection limit for clients in Petasos config?,\"To prevent infinite loops or unintended consequences, there\\'s a limit on the number of HTTP redirections that clients can follow. The Xmidt Device API enforces a maximum redirection limit of 5. If a client encounters more than 5 redirections for a single request, it might indicate an issue with the configuration or service discovery mechanism.\"\\nWhat is the redirect API used by Petasos? Mention its http type as well.,\"Petasos has only one main API, that it uses to redirect incoming requests to the right Talaria. It is of http type POST and is /api/v2/device/send\"\\nWhat different Xmidt components talk to Petasos within the Xmidt or RDK Ecosystem?,\"Petasos only receives requests from either Talaria or Scytale. The two types of requests are:\\n- from a device looking to connect to talaria, and \\n- from scytale looking to forward a request to a device.\"\\nWhat is the health API in Petasos?,The GET /health API is to fetch the current health status of Petasos component.\\nWhat is the metrics API in Petasos?,The GET /metrics API is to fetch the current metrics being published by Petasos component.\\nWhat are some important metrics published by Petasos? What metrics may Prometheus scrape when it discovers the metric endpoint for Petasos?,\"Petasos publishes some important metrics around connections, requests & service discovery that describe the state of the microservice at that point. These are:\\n\\n1. api_requests_total(code 2xx to 3xx): no of requests received on a service\\n2. api_requests_total(code 4xx to 5xx): No of failed requests\\n3. in_flight_requests: No of the current request which is under processing\\n4. active_connections: The number of active connections associated with� the server\\n5. rejected_connections: The total number of connections rejected due to exceeding the limit\\n6. request_duration_seconds: latencies for requests\\n7. request_size_bytes: request sizes for requests.\\n8. response_size_bytes: response sizes for requests.\\n9. time_writing_header_seconds: latencies for writing HTTP headers.\\n10. maximum_processors: number of current maximum processors this processes is allowed to use\\n11. sd_instance_count: Service discovery instance count�\\n12. sd_error_count: Consul service discovery error count\\n13. sd_update_count: The total count of updates from the service discovery backend for a particular service\\n14. sd_last_error_timestamp: At what time last error occurred.\\n15. sd_last_update_timestamp: At what time the last service is updated.\"\\nHow do I deal with Petasos showing inconsistent TLS between Talaria and Petasos ? Does any fix exist for this? ,\"The most common reasons for inconsistent TLS is conflicting TLS configuration. \\n\\nTo fix this:\\nChange Talaria and Petasos Configuration to be consistent with http or https.\"\\nWhat are the different APIs available within Scytale?,\"Scytale has 2 main APIs or endpoints that are exposed:\\n\\n1. GET  /api/v3/device/{deviceID}/stat : Get Device Statistics\\n2. POST /api/v3 /device : Send WRP message to Device\"\\nWhat does the /device/{deviceID}/stat API do in Scytale? What error may I face when executing this API.,\"The /api/v3/device/{deviceID}/stat API belongs to Scytale, and is of http type GET. It is used to return the the statistics of the connected device, including information such as uptime and bytes sent. This information is retrieved from the Talaria that the device is connected to. If the Device is not connected then 404 is returned. Note that the deviceID should be like  mac:112233445566\"\\nWhat does the /device API do in Scytale? Is it used to communicate with a router device?,\"The /api/v3/device API belongs to Scytale, and is of http type POST. It is used to send Send Web routing protocol (WRP) message to Device or Router. Accepts a WRP message encoded in a valid WRP representation\\n(generally msgpack or json). Scytale then forwards the request to the correct Talaria. If the Device is not connected then 404 is returned.\"\\nWhat is the health API in Scytale?,The GET /health API is to fetch the current health status of Scytale component.\\nWhat is the metrics API in Scytale?,The GET /metrics API is to fetch the current metrics being published by Scytale component.\\nWhat are some configs or configurations that I can tweak in Scytale?,\"Scytale\\'s config file supports modification of certain configs that influence how Scytale handles requests:\\n\\n\"\"1. fanout.endpoints: godoc.org/github.com/xmidt-org/webpa-common/xhttp/fanout#Configuration\\n\\nif Empty it will be coming through the service discovery (Default Value: NONE)\"\"\\n2. fanout.authorization: Basic Auth token to use in each request (Default Value: NONE)\\n3. fanout.transport.maxIdleConns: Max idle connections. 0 Means no limit (Default Value: 0)\\n4. fanout.transport.maxIdleConnsPerHost: Max Idle connections per host.  (Default Value: 100)\\n5. fanout.transport.idleConnTimeout: Idle connection timeout period. 0 means no connection timeout (Default Value: 0)\\n6. fanout.fanoutTimeout: fanoutTimeout is the timeout for the entire fanout operation.fanoutTimeout should be the same or greater than the clientTimeout. fanoutTimeout includes multiple http requests. (Default Value: 45s)\\n7. fanout.clientTimeout: http client timeout (Default Value: 30s)\\n8. fanout.concurrency: concurrency is the maximum number of concurrent fanouts allowed. (Default Value: 1000)\\n9. fanout.maxRedirects: maxRedirects defines the maximum number of redirects each fanout will allow. (Default Value: unlimited)\\n10. fanout.redirectExcludeHeaders: are the headers that will *not* be copied on a redirect. (Default Value: None)\\n11. authheader: the basic auth token incoming api requests like /stat, and /devices (Default Value: None)\\n12. jwtValidator.keys.factory.uri: Themis endpoint for key (Default Value: None)\\n\"\"13. jwtValidator.keys.purpose: github.com/xmidt-org/bascule/blob/main/key/purpose.go\\n\\n0 ? Sign (require public key only)\\n1 ? Verify (requires private key only)\\n2 ? Encrypt (requires  private key only)\\n3 ? Decrypt (requires public key only ) (Default Value: None)\"\"\\n14. jwtValidator.keys.updateInterval: When to refresh the key from Themis (Default Value: None)\"\\nWhere is the configuration file for Scytale situated? Is this public?,\"Scytale\\'s config file is available on Github at the following URL:\\ngithub.com/xmidt-org/scytale/blob/master/scytale.yaml\"\\nWhat are some important metrics published by Scytale for Prometheus to scrape?,\"Scytale publishes some important metrics around request performance, service discoveries, and capability checks. These are exposed on the /metrics endpoint so that Prometheus can scrape these :\\n\\n1. auth_capability_check(outcome=\"\"Accepted\"\"): \\n2. auth_capability_check(outcome=!\"\"Accepted\"\"): \\n3. auth_validation: Counter of Auth validation success/failed.\\n4. sd_instance_count: Service discovery instance count.\\n5. sd_error_count: Consul service discovery error count\\n6. sd_update_count: The total count of updates from the service discovery backend for a particular service\\n7. sd_last_error_timestamp: At what time last error occurred.\\n8. sd_last_update_timestamp: At what time the last service is updated.\\n9. received_wrp_message_total: Total no of WRP Messages received� in scytale\\n10. api_requests_total(code 2xx to 3xx): no of requests received on a service\\n11. api_requests_total(code 4xx to 5xx): No of failed requests\\n12. in_flight_requests: No of the current request which is under processing\\n13. active_connections: The number of active connections associated with� the server\\n14. rejected_connections: The total number of connections rejected due to exceeding the limit\\n15. request_duration_seconds: latencies for requests\\n16. request_size_bytes: request sizes for requests.\\n17. response_size_bytes: response sizes for requests.\\n18. time_writing_header_seconds: latencies for writing HTTP headers.\\n19. maximum_processors: number of current maximum processors this processes is allowed to use\\n\"\\nWhat does a 404 response in Scytale\\'s API indicate? How may one be it comes up? able to fix the 404 response when,\"404 is a pretty common error scenarios, which indicates that the device is not connected to the cluster. This may be happening because Parodus (ie the component in CPE connecting to Talaria) has an incorrrect URL. To fix this, one must validate that Parodus holds the correct URL at its end. \"\\nWhat\\'s Talaria\\'s primary function? Explain the purpose of Talaria when talking about Xmod,\"Talaria is a critical service of the Xmidt ecosystem that enables communication between Xmidt Cloud clients (devices/routers/CPEs) and the backend itself. It manages millions of device connections, forwards device events, and sends/receives requests from devices.\"\\nHow does Talaria communicate with devices? What is the benefits of webhooks for Talaria. ,\"Talaria utilizes secure WebSocket connections to communicate with devices. WebSockets provide a persistent, bi-directional communication channel, ideal for real-time data exchange. Additionally, Talaria leverages WRP messages, a data format specifically designed for efficient communication within Xmidt Cloud.\"\\nWhat is the role of Talaria service?,Talaria service ensure continuous connections with devices and facilitate message exchange between routers and the cloud.\\nCan Talaria generate events?,\"Yes, Talaria can not only forward device-generated events but also originate its own events. These events might be triggered by internal Talaria actions or specific conditions within the system. Regardless of the source, Talaria sends these events to Caduceus, the event delivery service within Xmidt Cloud, for further processing and distribution to interested consumers.\"\\nHow does Talaria handle device authorization? Can Talaria also reject requests?,\"Talaria acts as a gatekeeper for device access to Xmidt Cloud services. It performs JWT (JSON Web Token) authorization using tokens issued by Themis, a security component within Xmidt Cloud. This authorization check ensures that a device is authorized to send data (GET/SET requests) before Talaria processes its requests. If the JWT check fails, Talaria rejects the request.\"\\nHow is a device session established with Xmidt Cloud or RDK Backend?,\"Shortly after booting up, a device takes the initiative to establish a secure WebSocket connection with Talaria (Xmidt Cloud). During this connection handshake, Talaria performs critical steps: \\n1. Authorization Check: Talaria validates the device\\'s authorization using the JWT token included in the connection request. If the authorization check fails, the connection attempt is rejected. \\n2. Message Routing: Upon successful authorization, Talaria establishes a bi-directional communication channel with the device. Now, Talaria can route messages between the device and Xmidt Cloud services.\"\\nWhat happens if a device fails authorization with Talaria?,\"If a device\\'s JWT authorization check fails during the session establishment process, Talaria responds with a specific message indicating a 401 (Unauthorized) status. This response might be accompanied by a session disconnection, preventing unauthorized devices from accessing Xmidt Cloud services.\"\\nHow does the authorization model work for device sessions with Talaria?,\"Authorization with Talaria primarily occurs during the initial session setup and periodically afterward. This approach avoids excessive credential validation for each request/event, improving scalability for large device deployments.\"\\nOut of Talaria and the Router who  initiates the session connection?,The device always initiates the WebSocket connection to establish a session with Xmidt Cloud (Talaria).\\nWhat is the role of ping/pong messages for Talaria in a session?,\"Once a session is established, devices must periodically send ping messages to Talaria, which responds with pong messages.  These ping messages act as a heartbeat mechanism. This mechanism verifies the aliveness of both parties in the connection.\"\\nWhat are the Talaria API endpoints for device interaction?,\"Talaria offers three primary API endpoints for interacting with connected devices: \\n1. Device Statistics: /device/{deviceID}/stat : Function: Provides statistics about a specific device identified by its ID. \\n2. Get Devices:  /devices : Function: Retrieves a list of all connected devices. (Not to be used in production environment due to potential scalability issues)\\n3. Send WRP to Device: /device/send : Function: Allows sending WRP messages directly to a specific device.\"\\nWhy shouldn\\'t Routers or Xmidt Cloud clients directly query Talaria?,\"CPES or Routers or Xmidt Cloud clients should avoid directly querying Talaria for device information or sending messages. Instead, they are encouraged to route requests through WebPA (Tr1d1um), the official Xmidt Cloud API, which provides a more controlled and secure communication channel.\"\\nHow can Talaria control connected routers or devices?,\"Talaria offers a built-in control server exposing functionalities for managing connected devices. This control server allows for: \\n1.  /device/gate : To Gate devices. Controls whether specific devices can connect to this particular Talaria instance, essentially allowing or denying device connections. \\n2.  /device/drain : To drain devices.  Removes connected devices from the Talaria instance, effectively disconnecting them.\"\\nWhat role does JWT play in device interaction with Talaria? At what point is WJWA,\"Talaria utilizes JWT (JSON Web Token) for basic device authentication and to prevent potential spoofing attempts. Here\\'s a breakdown: \\n* JWT Claims: JWT claims are used to compare specific device attributes like MAC address, serial number, and transaction UUID with corresponding headers included in the connection request. \\n\\n* Optional JWT Usage: While JWT is not mandatory, it can be used to verify the legitimacy of a device by comparing claim data with header attributes. The primary purpose is to prevent devices from impersonating others. \\n\\n* Validation Scope: Talaria validates JWT tokens during the initial WebSocket connection establishment, not for every single request sent through the connection. Once the connection is established, Talaria relies on internal mechanisms to manage and filter traffic.\"\\nHow does JWT prevent spoofing?,\"Even though JWTs are not encrypted, they are cryptographically signed. This signature verification helps prevent a malicious actor from impersonating another device. A bad actor could potentially exploit a compromised JWT, but only to impersonate that specific device, not any other device in the system.\"\\nWhat role does JWT play in device interaction with Talaria?,\"Talaria utilizes JWT (JSON Web Token) for basic device authentication and to prevent potential spoofing attempts. Here\\'s a breakdown: \\n\\n- JWT Claims: JWT claims are used to compare specific device attributes like MAC address, serial number, and transaction UUID against corresponding headers included in the connection request.\\n- Optional JWT Usage: While JWT is not mandatory, it can be used to verify the legitimacy of a device by comparing claim data with header attributes. The primary purpose is to prevent devices from spoofing or impersonating others.\\n- Validation Scope: Talaria validates JWT tokens during the initial WebSocket connection establishment, but not every single request sent through the connection. Once the connection is established, Talaria relies on internal mechanisms to manage and filter traffic.\"\\nWhat is the message validation approach in Talaria?,\"Talaria employs selective message validation. It focuses on validating the WebSocket connection itself during establishment, where JWT verification might occur. Once the connection is established, Talaria relies on its internal mechanisms to manage and potentially filter traffic flowing through that connection. This approach aims to balance security with scalability considerations.\"\\nWhat is the message validation approach in Talaria?,\"Xmidt Cloud operates under the assumption of trust for internal services. Once a message reaches Talaria, any component receiving the message upstream is expected to implicitly trust the sender. An alternative approach involving cryptographically signed messages for validation by upstream consumers is mentioned but considered less scalable for the vast number of messages within Xmidt Cloud.\"\\n\"What information does the \"\"Convey Token\"\" contain in Talaria?\",\"The \"\"Convey Token\"\" includes metadata associated with the device. This metadata is likely passed by Parodus as HTTP headers within the connection request.\"\\n\"How does Talaria handle the \"\"Convey Token\"\"?\",\"Talaria extracts the metadata from the \"\"Convey Token\"\" and constructs a \"\"Convey Object.\"\" This object is then added to device options for further processing within Talaria.\"\\n\"How is the \"\"Convey Token\"\" transmitted?\",\"The \"\"Convey Token\"\" is provided by Parodus in two ways: \\n1. HTTP Header: Parodus includes it as a request attribute named \"\"X-Webpa-Convey\"\" within the connection request. \\n2. Logs: The \"\"Convey Token\"\" is also persisted in logs maintained by Petasos Server, Talaria events, and Talaria server logs (at \"\"info\"\" level).\"\\n\"If CPE ws connection is closed by any reason  and at the same time \\nwe had received a request to apply some configurations on the CPE \\nwhat will be happening in this case?\",\"CPE has a  logic to connect to talaria which is in infinite loop unless it is connected.\\n\\nDuring any request to apply some configuration to CPE while it is retrying connection:\\n1. if CPE is not connected  then 404 will be returned to user.\\n2. There is no need to Queue the data and apply it later when the CPE is online again, since Tr1d1um has a retry mechanism \"\\nWhat APIs are exposed by Talaria to gate or limit devices to Xmidt?,\"Talaria can be set to disallow incoming websocket connections. When the gate is closed, all incoming websocket connection requests are rejected with a 503 status.\\n\\nTalaria always starts with the gate open, allowing new websocket connections. Already connected websockets are not affected by closing the gate.\\n\\nThe RESTful endpoint that controls this is /api/v2/device/gate.\\n\\nThe following APIs or endpoints are exposed by Talaria to gate incoming connections:\\n\\n1. /device/gate : used to open or close a gate. Can be of http type PUT, POST or PATCH. If the gate is open then only it will be accepting the WS connections else it will be returning 503 status.\\n2. /device/gate?open=<boolean> : used to fetch current status of gate, whether its open or closed. Can be of http type GET. Any attempt to open the gate when it is already open or close it when it is already closed results in a 200 status. If this endpoint did change the status of the gate, a 201 status is returned.\\n\"\\nWhat are the different APIs are exposed by Talaria to drain router connections?,\"Talaria supports the draining of websocket connections. Essentially, this means shedding load in a controlled fashion.\\n\\nWebsocket connections can be drained at a given rate, e.g. 100 connections/minute, or can be drained as fast as possible.\\nOnly 1 drain job is permitted to be running at any time. Attempts to start a drain when one is already active results in an error.\\n\\nWe need to close the device gate so that it will no accept the new connections request and we can drain the connections as per our need.\\nThe RESTful endpoint that controls the connection drain is /api/v2/device/drain.\\n\\n\\nThe following APIs or endpoints are exposed by Talaria to drain connections:\\n\\n1. /device/drain : when used with http type GET, then this will return the current status of the drain job whether the job is active and previous drain job status\\n2. /device/drain : when used with http type PUT, POST, or PATCH, then this will register a job at the server with the given inputs, and then attempts to start a drain job. If no parameters are supplied, all devices are drained as fast as possible.\\n3. /device/drain : when used with http type DELETE, then this will halt or cancel the running drain job. \"\\n\"What parameters may be supplied to a PUT, POST, or PATCH call to the /device/drain endpoint so as to customize the drain behaviour?\",\"Several parameters may be supplied to customize the drain behavior of a Talaria node or service. \\n1. count: indicates the maximum number of websocket connections to close.\\n2. percent: indicates the percentage of connections to close. has a higher priority than count when specified together.\\n3. rate: indicates the number of connections per unit time (tick) to close. If this value is not supplied, connections are closed as fast as possible.\\n4. tick: indicates the unit of time for rate. If rate is supplied and tick is not supplied, a tick of 1s is used. If rate is not supplied and tick is supplied, tick is ignored. When Tick is provided with Rate then it will drop the no of connections per Tick time. When Nothing is provided it will be dropping the WS connections as fast as possible\"\\nWhat are Talaria\\'s API endpoints for interacting with devices?,\"Talaria offers three API endpoints for interacting with routers or devices. Below 3 APIs need a JWT token for authentication, else 403 is returned.\\n\\n1. /device/{deviceID}/stat - Returns the statistics of the connected device, including information such as uptime and bytes sent. It is of http type GET. This request does not communicate with the device, instead the request returns stored statistics that are stored \"\"in-memory\"\" of the talaria instance.\\n2. /devices - Used to return a list of all the actively connected devices and their statistics, just like the stat command. Is of http type GET. This API is not recommended for use in production as it is an expensive call.\\n3. /device/send - Http type is POST. API is used to Send WRP messages to a specific device. Talaria will accept a WRP message encoded in a valid WRP representation - generally msgpack or json. If the message is json encoded, talaria will encode the payload as msgpack. If the device returns a message, it will be encoded as the HTTP accept header.\"\\nWhat is the health API in Talaria?,The GET /health API is to fetch the current health status of Talaria component.\\nWhat is the metrics API in Talaria?,The GET /metrics API is to fetch the current metrics being published by Talaria component.\\nWhat are some configs or configurations that I can tweak in Talaria to influence how it manages connections and devices?,\"Following are some important configurables in Talaria\\'s config file when it comes to storing webhooks:\\n\\n1. device.manager.maxDevices: maximum number of devices allowed to connect to this talaria (Default Value: 4294967295)\\n2. device.manager.deviceMessageQueueSize: the capacity of the channel which stores messages waiting to be transmitted to a device. (Default Value: 100)\\n3. device.manager.pingPeriod: the time between pings sent to each device to ensure they are still reachable. (Default Value: 45s)\\n4. device.manager.idlePeriod: the length of time a device connection is allowed to be idle, with no traffic coming from the device. (Default Value: 45s)\\n5. device.manager.requestTimeout: the timeout for all inbound HTTP requests (Default Value: 30s)\\n6. device.outbound.method: http method to use against the receiving server. (Default Value: POST)\\n7. device.outbound.retries: the number attempts to sent the message to the receiver. (Default Value: 1)\\n8. device.outbound.eventEndpoints.default: map defining where to send the events to (Default Value: http://caduceus:6000/api/v3/notify)\\n9. device.outbound.eventEndpoints.EventType: https://godoc.org/github.com/xmidt-org/webpa-common/device#EventType (Default Value: )\\n\"\"10. device.outbound.enableConsulRoundRobin: If this is true. will overwrite the eventEndpoints with using consul to discover the caduceus in the datacenter.\\nif no services are found, talaria will fail back to the defined endpoint. (Default Value: FALSE)\"\"\\n11. device.outbound.requestTimeout: how long an event will be held on to starting from when it is received till completing the http request. (Default Value: 125s)\\n12. device.outbound.defaultScheme: the default scheme for the http request. (Default Value: https)\\n13. device.outbound.allowedSchemes: list of schemes, used in conjunction with the defaultScheme (Default Value: [\"\"https\"\"])\\n14. device.outbound.outboundQueueSize: the size of the buffer to queue messages for each receiver. (Default Value: 1000)\\n15. device.outbound.workerPoolSize: configures how many active go threads send messages to the receivers. (Default Value: 100)\\n16. device.outbound.transport.maxIdleConns: No limit at all (Default Value: 0)\\n17. device.outbound.transport.maxIdleConnsPerHost:  (Default Value: 100)\\n18. device.outbound.transport.idleConnTimeout: do not timeout while in idle (Default Value: 0)\\n19. device.outbound.clientTimeout: specifies a time limit for requests made by this Client. The timeout includes connection time, any redirects, and reading the response body. The timer remains running after Get, Head, Post, or Do return and will interrupt reading of the Response Body. (Default Value: 160s)\\n20. device.outbound.authKey: the basic auth token used for sending messages to the receiver. (Default Value: None)\\n21. inbound.authKey: the basic auth token incoming api requests like /stat, and /devices (Default Value: None)\\n\"\\nIs there any PII or Personally Identificable Information available inside Talaria Logs?,\"Yes, we have confidential user information being printed in Talaria Logs when DEBUG log-level I used. Information like deviceId which is the user\\'s routers\\'s mac address is PII and should be handled carefully.\"\\nWhat are some important metrics published by Talaria for scraping by Observability tools?,\"Observability tools like Prometheus are responsible for scraping metrics exposed by microservices in the backend ecosystem, storing them in a TSDB (time series data base) for querying, reporting and alerting. Talaria\\'s code is configured as such to expose information related to its connections, hashes, requests, and service discovery. Following are the metrics exposed by Talaria on port 6204: \\n\\n1. api_requests_total(code 2xx to 3xx): no of requests received on a service\\n2. api_requests_total(code 4xx to 5xx): No of failed requests\\n3. in_flight_requests: No of the current request which is under processing\\n4. active_connections: The number of active connections associated with� the server\\n5. rejected_connections: The total number of connections rejected due to exceeding the limit\\n6. request_duration_seconds: latencies for requests\\n7. request_size_bytes: request sizes for requests.\\n8. response_size_bytes: response sizes for requests.\\n9. time_writing_header_seconds: latencies for writing HTTP headers.\\n10. maximum_processors: number of current maximum processors this processes is allowed to use\\n11. outbound_inflight: Currently active requests from devices to caduceus\\n12. outbound_request_duration_seconds: The durations of outbound requests from devices\\n13. outbound_requests: How many outbound requests is made\\n14. outbound_queue_size: Events will be sent from this queue to the caduceus\\n15. outbound_dropped_messages: How many messages are dropped in talaria\\n16. outbound_retries: How many retires it has to before dropping a WRP message\\n17. gate_status: 0 means the gate is closed, 1 mean gate is open\\n18. drain_status: 1 means drain job is running\\n19. drain_count: How many devices are drained\\n20. inbound_wrp_messages: Stats of devices which are connected/not connected to talaria due to Device access check\\n21. device_count: Currently, active CPE which are connected to talaria\\n22. duplicate_count: Count of Duplicate devices request in talaria\\n23. request_response_count: How many SimpleRequestResponseMessageType calls are made to CPE\\n24. ping_count: How many pings has happened\\n25. pong_count: How many pongs has happened\\n26. connect_count: How many CPE\\'s are connected till now\\n27. disconnect_count: How many CPE\\'s are Disconnected till now\\n28. device_limit_reached_count: How many devices are rejected after the limit is reached.\\n29. hardware_model: HW Model, FW version, partner_id, and trust wise stats of devices\\n30. wrp_source_check: Stats of WRPMessage source which is coming from CPE\\n31. rehash_keep_device: How Many CPE\\'s are present on this talaria\\n32. rehash_disconnect_device: How many CPE are disconnected from this talaria\\n33. rehash_disconnect_all_count: Due to any error if talaria is dropping all the devices.\\n34. rehash_timestamp: When the rehashing started\\n35. rehash_duration_ms: how long rehashing ran\\n36. sd_instance_count: Service discovery instance count is less than 1\\n37. sd_error_count: Consul service discovery error count\\n38. sd_update_count: The total count of updates from the service discovery backend for a particular service\\n39. sd_last_error_timestamp: At what time last error occurred.\\n40. sd_last_update_timestamp: At what time the last service is updated.\\n41. auth_validation: Counter of Auth validation success/failed.\"\\nHow does rehashing work in case of Talaria?,\"Talaria supports rehashing via service discovery, which can be done either by Consul or Zookeeper. In case of Consul, when a new Talaria instance is added, rehashing takes place. The flow is:\\n\\n1. On startup, every talaria will self-register itself and consul.\\n2. Once talaria is successfully registered in consul we will be getting an event in the talaria.\\n3. Based on that event internal consistent hash table will be updated in talaria\\n4. Few CPE WS connections will be dropped which should not be on this talaria instance.\\n5. Again CPE will try to get talaria information from petasos\\n6. Petasos will be sharing new talaria information with the CPE\\n7. CPE will be connecting to the new talaria instance now.\"\\nWhich file in Talaria\\'s code is responsible for handling the rehashing?,\"The rehasher.go file in Talaria\\'s opensource code is responsible for managing the rehashing logic. It is situated in the webpa-common repository. The code is written in Golang, and is available on Github. The URL to the specific file is: https://github.com/xmidt-org/webpa-common/blob/main/device/rehasher/rehasher.go\"\\nIs it possible to simulate a device to test connection behaviour with RDK or Xmidt backend?,\"Yes, one can either use the RDK-B Simulator or a Kratos client to simulate or mock a device that would connect to RDK Backend or the Xmidt cluster. The core part of the device simulator that connects to Talaria is Parodus. \\n\\nThe difference between Kratos and RDK-Simulator is that kratos can mock Parodus (Xmdit Agent on the CPE), whereas RDKB Device Simulator mocks an entire RDK-B device.\"\\nWhat is Kratos with respect to RDK and Xmidt ecosystem?,\"Kratos is a websocket wrapper that provides a simple API for making new connections, sending messages on that connection, and providing a way to handle received messages. It\\'s codebase is opensource and available on Github at https://github.com/xmidt-org/kratos\"\\nWhat may be happening when Talaria is not showing up in Consul?,\"Talaria is responsible for registering itself with Consul at startup. To check if a talaria instance is successfully registered with Consul for service-discovery, one can run this command on a Consul node to test if Talaria is registered or not:\\nconsul catalog services\\n\\nIf Talaria does not show up in the list, then validate Consul\\'s client block in Talaria\\'s config, with correct TLS and address. \"\\nWhat may be happening when Caduceus is not receving any events from Talaria?,\"If it has been detected that Caduceus is not receiving events, then one must update the Talaria\\'s config yaml with the correct information of caduceu\\'s FQDN, port and its /api/v3/notify endpoint. \"\\nIs there any PII or Personally Identificable Information Available in Themis\\' Logs?,There is no PII (personally identifiable information) printed in Themis\\' logs. However do not that using DEBUG log-level in Prod is not recommended.\\nWhat are some configs or configurations that I can tweak for Themis? I would like to customize the behaviour of how tokens generated by Themis are treated.,\"Themis allows configuring certaing values that influence how it generates and treats tokens. These values are available in Themis\\' opensource code in Github on https://github.com/xmidt-org/themis/blob/main/themis.yaml. Some of these config values are:\\n\\n1. token.alg: Alg is the required JWT signing algorithm to use (Default Value: RS256)\\n2. token.nonce: Nonce indicates whether a nonce (jti) should be applied to each token emitted by this factory. (Default Value: FALSE)\\n3. token.duration: Duration specifies how long the token should be valid for. An exp claim is set using this duration from the current time if this field is positive. (Default Value: NONE)\\n4. token.notBeforeDelta: NotBeforeDelta is a golang duration that determines the nbf field. This field is parsed and added to the current time at the moment a token is issued. The result is set as an nbf claim. Note that the duration may be zero or negative. (Default Value: NONE)\\n5. token.claims: Claims is an optional map of claims to add to every token emitted by this factory. Any claims here can be overridden by claims within a token Request. (Default Value: NONE)\\n6. token.metadata: Metadata describes non-claim data, which can be statically configured or supplied via a request. Will be useful while sending the request to remote server. (Default Value: NONE)\\n7. token.remote.method: Method is the HTTP method used to invoke the URL (Default Value: POST)\\n8. token.remote.url: URL is the remote endpoint that is expected to receive Request.Metadata and return a JSON document which is merged into the token claims (Default Value: NONE)\\n9. token.key.kid: Kid is the key id to use initially. If unset, the name of the key is used. Note that the kid can change is the key is rotated or updated during application execution. (Default Value: NONE)\\n10. token.key.type: Type indicates the type of key. This field dictates both how the key File is read or how the key is generated. (Default Value: RSA)\\n11. token.key.bits: Bits indicates the bit size for a generated key (Default Value: NONE)\\n12. token.key.file: File is the system path to a file where the key is stored. If set, this file must exist and contain either a secret or a PEM-encoded key pair. If this field is not set, a key is generated. (Default Value: NONE)\"\\nWhat is WebPA within the Xmidt ecosystem? What other name is WebPA also known as?,WebPA is a software component within Xmidt that facilitates communication with devices adhering to the TR-181 data model. It is also referred to as Tr1d1um.\\nWhat functionalities does the WebPA server or Tr1d1um offer?,\"Tr1d1um offers functionalities like:\\n\\n- RESTful API: Exposes a RESTful API for interacting with devices using HTTP requests.\\n- TR-181 Translation: Translates between the WebPA API and the TR-181 data model, enabling communication with TR-181 devices. \\n- Device Configuration Management: Allows managing device configurations through GET/SET operations on TR-181 parameters. \\n- Event Subscriptions: Provides mechanisms for subscribing to receive notifications about device events.\"\\nWhat\\'s the historical context behind WebPA?,\"WebPA was initially designed as a combined service encompassing both core Xmidt routing and TR-181 translation functionalities. However, after large-scale deployments, a decision was made by Comcast to separate these functionalities. The core routing aspects remained within the Xmidt project, while the translation layer evolved into the independent WebPA (Tr1d1um) microservice.\"\\nHow can Tr1d1um be used for device system configuration?,\"Tr1d1um or WebPA empowers Xmidt Cloud to manage device configurations remotely. It allows: \\n* Reading Configuration: Retrieving current device configuration settings based on TR-181 parameters. \\n* Updating Configuration: Sending commands to devices to modify their configurations according to the TR-181 standard.\"\\nHow does Tr1d1um collect device information?,\"WebPA or Tr1d1um facilitates the collection of ad-hoc device information from connected devices. Devices actively push data like firmware version, XPC configuration details, model, serial number, and boot time to the server. This information is then stored in server statistics and logs for analysis.\"\\nWhat\\'s the purpose of collecting device information by Tr1d1um?,\"WebPA-enabled device information collection serves several purposes: \\n* Real-time Data Gathering: Provides near real-time access to device data for analytical purposes. \\n* Service Level Assurance: Helps ensure that Xmidt Cloud services are meeting performance expectations. \\n* Customer Experience Optimization: Contributes to improving the overall customer experience by identifying potential issues and areas for improvement.\"\\nHow does WebPA interact with TR-181 devices for configuration?,\"Tr1d1um or WebPA acts as an intermediary between Xmidt Cloud services and TR-181 devices when managing configurations. It translates between the WebPA API used by Xmidt Cloud and the TR-181 data model understood by the devices. This translation enables Xmidt Cloud to send configuration commands (SET) or retrieve current settings (GET) using the WebPA API, which WebPA then translates into TR-181 commands for the devices to understand.\"\\nHow can a device be identified in WebPA?,\"Tr1d1um supports various formats for device identification using the X-WebPA-Device-Name header. Here are the accepted formats: \\n- uuid: {uuid}/{service}/{ignored} \\n- dns: {hostname}/{service}/{ignored} \\n- mac: {mac address}/{service}/{ignored} \\n- serial: {serial number}/{service}/{ignored}\"\\nWhat\\'s important about device identification formats in Tr1d1um?,\"The format chosen for device identification impacts routing. Formats relying on MAC address or serial number (mac:{mac address} or serial:{serial number}) require the device name to be pre-registered. Without pre-registration, routing to the device fails.\"\\nWhat information does the WebPA server extract from the device name?,\"WebPA primarily focuses on using the device name for routing purposes. Additional details extracted might depend on the specific format used (e.g., serial number from serial:{serial number}).\"\\nHow are device name and group membership conveyed within Tr1d1um? What http headers are needed for secure websocket content usage?,\"During Secure WebSocket (WSS) connection establishment, HTTP headers are used to share device information: \\n* X-WebPA-Device-Name (Required): This header specifies the device name, ideally in the format serial-{serial number}. \\n* X-WebPA-Group-Membership (Optional): This optional header contains a comma-separated list of groups the device belongs to.\"\\nWhat is Tr1d1um\\'s role in device communication? What different APIs help enabling these functionalities.,\"Tr1d1um (WebPA) acts as a bridge for communication with TR-181 compliant devices. It facilitates functionalities like: \\n- /stat : Device Statistics Retrieval: Allows fetching statistics (e.g., uptime) for connected devices using the /stat endpoint. \\n- /config : Device Configuration Management: Enables CRUD (Create, Read, Update, Delete) operations on device configurations through the /config endpoints. Tr1d1um validates incoming requests, prepares WRP messages, and interacts with the Xmidt Cloud API for configuration management.\\n- /hook(s) : Event Subscription: Provides mechanisms for registering event listeners using /hooks endpoints to receive notifications about published Xmidt Cloud events.\"\\nHow does Tr1d1um handle CRUD operations for device configuration?,\"When a configuration request arrives, Tr1d1um performs the following actions: \\n1. Validation: Tr1d1um validates the incoming request to ensure its validity. \\n2. WRP Message Construction: Tr1d1um encapsulates the validated request data into a SimpleRequestResponse WRP message, a specific message format used within Xmidt Cloud. \\n3. Message Encoding: Tr1d1um encodes the constructed WRP message using msgpack, a compact data serialization format, before sending it to the Xmidt Cloud API.\"\\nHow does Tr1d1um or WebPA authenticate back-end integrations?,\"WebPA utilizes OAuth 2.0 \"\"Client Credential Flow\"\" for back-end integration authentication. Here\\'s a breakdown: \\n\\n- Token Request: RDK Back-End requests an access token using its OAuth 2.0 credentials. \\n- Token Type: The access token uses the \"\"bearer\"\" type. \\n- Token Format: The access token is a JSON Web Token (JWT) signed with the SHA256 algorithm. \\n- JWT Claims: The JWT contains standard claims like audience (aud), expiration time (exp), and a unique JWT identifier (jti). \\n- Token Expiration: Access tokens expire after 24 hours (86400 seconds). \\n- Transport Security: Communication should leverage Transport Layer Security (TLS) 1.3 with mutual TLS for added security.\"\\nWhat is WRP and what does it do?,WRP (Web Routing Protocol) acts as a core message envelope within Xmidt. It provides a standardized format for messages exchanged between devices and various Xmidt services. WRP facilitates routing messages efficiently across the Xmidt cloud infrastructure.\\nWhy is WRP important for Xmidt?,\"Xmidt interacts with a vast number of devices using diverse communication protocols (WebSocket, HTTP, etc.). WRP offers a unified message structure that transcends these protocols, enabling seamless communication and message processing within Xmidt, regardless of the underlying transport mechanism.\"\\nHow does message encoding work in WRP?,\"WRP messages are always encoded using MsgPack for efficient data transmission. This happens regardless of the format (MsgPack, JSON, or HTTP headers) used by the initial API call. Internally, Xmidt services like Skytale convert incoming requests into MsgPack before sending them further.\"\\nAre there performance implications with WRP message encoding?,\"Yes, converting messages to MsgPack can introduce some overhead. If performance optimization is critical, directly emitting WRP messages in MsgPack format (like Tr1d1um does) can avoid the conversion step and potentially improve performance.\"\\nWhat\\'s the OSI layer stack for WRP communication?,\"WRP resides at a higher level within the communication stack. Here\\'s a breakdown (bottom to top): \\n- Physical Layer \\n- IP Layer \\n- TCP Layer \\n- TLS/SSL Layer \\n- HTTP/WebSocket Layer (depending on the protocol) \\n- WRP Layer \\n- Application Layer (Xmidt services)\"\\nWhat are Tr1d1um\\'s API endpoints for fetching device statistics?,Tr1d1um exposes the /stat endpoint or API to fetch the statistics (i.e. uptime) for a given router connected to the Xmidt. This endpoint is a simple shadow of its counterpart on the Xmidt Cloud API (Scytale). Tr1d1um simply passes through the incoming request to XMiDT as it comes and returns whatever response XMiDT provided.\\nWhat are Tr1d1um\\'s API endpoints for performing CRUD operations?,\"Tr1d1um exposes the /config endpoint or API to set/get multiple values, or to replace or delete entries from the table. The same API has different usage and behaviours when different http types are used:\\n\\n1. /config         // CRUD operations\\n1.1.    GET     /device/{device_id}/config          // Get multiple parameter values or attributes\\n1.2.    PATCH   /device/{device_id}/config          // Set multiple parameter values or attributes\\n1.3.    POST    /device/{device_id}/config/{table}  // Add entry to table\\n 1.4.   PUT     /device/{device_id}/config/{table}  // Replace table data\\n 1.5.   DELETE  /device/{device_id}/config/{row}    // Delete entry from table\\n\\nTr1d1um validates the incoming request, injects it into the payload of a SimpleRequestResponse WRP message and sends it to XMiDT.\\nTr1d1um encodes the outgoing WRP message in msgpack as it is the encoding XMiDT ultimately uses to communicate with devices.\"\\nWhat are Tr1d1um\\'s API endpoints relating to Event listener registration?,\"Devices connected to the XMiDT Cluster generate events (i.e. going offline). The webhooks library used by Tr1d1um publishes these events. These endpoints allow API users to setup listeners of desired events and fetch the current list of configured listeners in the system. The two important APIs for event listeners are:\\n1.  /hook : which of http type POST, and is used for registration of webpa listener\\n2. /hooks: which is of http type GET, and is used for retreival of registered webpa listeners.\"\\nWhat are some important metrics published by Tr1d1um for Prometheus to scrape?,\"1. api_requests_total(code 2xx to 3xx): no of requests received on a service\\n2. api_requests_total(code 4xx to 5xx): No of failed requests\\n3. in_flight_requests: �The Current request which is under processing\\n4. active_connections: The number of active connections associated with� the server\\n5. rejected_connections: The total number of connections rejected due to exceeding the limit\\n6. request_duration_seconds: latencies for requests\\n7. request_size_bytes: request sizes for requests.\\n8. response_size_bytes: response sizes for requests.\\n9. time_writing_header_seconds: latencies for writing HTTP headers.\\n10. maximum_processors: number of current maximum processors this processes is allowed to use\\n11. webhook_list_size_value: Tr1d1um is not able to get the webhooks from argus\\n12. chrysom_polls_total (outcome=\"\"success\"\"): Tr1d1um�is not able poll the webhooks from argus\\n13. chrysom_polls_total (outcome=\"\"failure\"\"): Tr1d1um�is not able poll the webhooks from argus\\n14. auth_capability_check(outcome=\"\"Accepted\"\"): \\n15. auth_capability_check(outcome=!\"\"Accepted\"\"): \\n16. auth_validation: Counter of Auth validation success/failed.\"\\nIs there any PII or Personally Identificable Information Available in Tr1d1um Logs?,\"Yes, we have confidential user information being printed in Tr1d1um Logs when DEBUG log-level I used. Information like deviceId which is the user\\'s routers\\'s mac address is PII and should be handled carefully.\"\\nWhat APIs are available for Tr1d1um to check its metrics and health?,\"There are 2 APIs available in Tr1d1um or WebPA for checking its metrics and health:\\n\\n1. GET /metrics: fetches different metrics available for Tr1d1um \\n2. GET  /health: fetches current microservice health for Tr1d1um\"\\nWhere is the configuration file for Caduceus available?,\"The config file for Caduceus is opensource, and available on Github at the following URL: github.com/xmidt-org/caduceus/blob/main/caduceus.yaml . This config-file is essentially a YAML config-map.\"\\nWhere is the configuration file for Tr1d1um available?,\"The config file for Tr1d1um is opensource, and available on Github at the following URL:github.com/xmidt-org/tr1d1um/blob/master/tr1d1um.yaml This config-file is essentially a YAML config-map.\"\\nWhere is the configuration file for Talaria available?,\"The config file for Talaria is opensource, and available on Github at the following URL: github.com/xmidt-org/talaria/blob/master/talaria.yaml This config-file is essentially a YAML config-map.\"\\nWhere is the configuration file for Scytale available?,\"The config file for Scytale is opensource, and available on Github at the following URL: github.com/xmidt-org/scytale/blob/master/scytale.yaml This config-file is essentially a YAML config-map.\"\\nWhere is the configuration file for Petasos available?,\"The config file for Petasos is opensource, and available on Github at the following URL: \\tgithub.com/xmidt-org/petasos/blob/master/petasos.yaml This config-file is essentially a YAML config-map.\"\\nWhere is the configuration file for Themis available?,\"The config file for Themis is opensource, and available on Github at the following URL: github.com/xmidt-org/themis/blob/main/themis.yaml This config-file is essentially a YAML config-map.\"\\nWhere is the configuration file for Argus available?,\"The config file for Argus is opensource, and available on Github at the following URL: github.com/xmidt-org/argus/blob/main/argus.yaml This config-file is essentially a YAML config-map.\"\\n\"Does the name \"\"Xmidt\"\" have a meaning\",\"No. Xmidt is based on the creators last name Weston Sch\"\"midt\"\"\"\\n\"What is the relation between WebPA, Xmidt and WRP?\",\"WebPA -> Protocol Agent (on the CPE)\\nWRP -> the applied protocol (Web Routing Protocol)\\nXmidt -> the \"\"Cloud Web Router\"\"\"\\nIsn\\'t Xmidt a common name for pretty much everything?,\"kind of, used synonymously with \\'Xmidt Cloud\\'\\n\\nXmidt = Xmidt Client (Parodus) + Xmidt Cloud + Xmidt \"\"Back End Systems\"\" (Tr1d1um and Codex covering \\'logic\\')\"\\nWhat Client of the Xmidt Clouds runs on the CPE?,\"Xmidt Cloud Client == also known as Parodus, is what runs on the CPEs or Routers.\"\\nHow does the CPE to Talaria connection flow look like?,\"The CPE to Talaria connection requires a lot of hops across multiple microservices in the Xmidt ecosystem. These steps are:\\n\\n1.  On startup CPE will be  requesting a token from themis on /issue endpoint.\\n2. Themis will sending the request to xmidt listener for the device level of checking weather this device is associated with Natco or not.\\n3. based on response from Resource Inventory� then will issue a token or return a HTTP 4XX error to CPE.\\n4. Now CPE will try to connect to petasos-rewriter.\\n5. petasos-rewriter will forward the request to petasos.\\n6. petasos will query in consul about healthy talaria information.\\n7. petasos will store talaria information in hash and return the talaria information to petasos-rewriter.\\n8. petasos-rewriter will update public talaria information in� the response body and header.\\n9. now CPE has the talaria information where it needs to connect.\\n10. talaria on startup will register himself in consul.\\n11. CPE will be opening a WebSocket connection to talaria.\\n12. talaria will validate the request based on configuration provided to it and will establish� a websokect connection to CPE if the request was valid.\\n13. talaria will send� a event to caduceus that device is connected.\\n14. talaria will be doing ping pong(activeness of websocket connection) after every X(45) seconds configurable.\\n15. if request was invalid CPW will� start again the process from step 2.\"\\nWhat is the step-wise flow when GET or SET of Paramters is done against a Router or Device?,\"A couple of hops are needed for Getting or Setting Parameters on a CPE or Router. These are:\\n\\n1. �Request will be coming for Tr1d1um, tr1d1um will validate the request and convert the request into WRPMessage and sends it to scytale.\\n2. scytale will get all healthy talaria from consul.\\n3. Scytale will fanout the HTTP request to all talaria instances.\\n4. if all talaria instances returned 404(device doesn\\'t exist) the same will be sent as response.\\n5. if device websocket was active on any of the� talaria instance.\\n6. talaria will send that WRP Message in websokcet.\\n7. CPE will�receive the WRPMessage and act on it and will return response to talaria.\\n8. talaria will return response to scytale.\\n9. scytale will return response to tr1d1um.\"\\nHow are webhooks registered? How are webhooks persisted?,\"Webhooks are rgeistered in Tr1d1um, however these webhooks are stored by Argus in YugabyteDB. \\n\\n1. We need to resgiter the webhook in Tr1d1um.\\n2. Tr1d1um will� validate the add WebhookRequest and send it to argus.\\n3. Argus will presist the webhook data in configured DB for the time defined by user.\"\\nHow does Event Delivery managed in Xmidt? ,\"Events are packed in WRP messages, and delivered to registered webhook endpoints. \\n\\n1. Caduceus will be polling for webhooks after X(5, configurable) seconds and will create the senders in memory.\\n2. Any incoming request to caduceus notify endpoint will place the WRP Message in the sender\\'s queue post successful validation.\\n3. Sender will send the WRP Message to the registered webhook endpoints. in below case we have a sender named as webpa data capture.\"\\n\"I am looking for a software platform that powers Gateways with DOCSIS, PON, DSL, 5G, and ethernet capabilities. It should be standardized and customizable, and support basic features such as routing, Wi-Fi, DNS,diagnostics, remote management, and IoT interfaces. What options do I have?\",\"For powering next-gen gateways across DOCSIS, PON, DSL, 5G, and ethernet access networks, once can consider leveraging RDK-B. Created and specified by Comcast, RDK, fully known as \"\"Reference Design Kit\"\" is a complete, fully integrated software bundle, specified by Comcast to run on QAM, IP-Only and Hybrid (QAM+IP) devices. It has 3 main \"\"profiles\"\", namely: RDK-B, RDK-V, and RDK-C. RDK-B is Reference Desk Kit for Broadband. RDK-V is Reference Design Kit for Videos, and RDK-C is Reference Design Kit for Cameras. \\nRDK-B stands out as a highly advantageous solution in the realm of broadband technology, offering a plethora of benefits that underscore its versatility and efficiency. RDK-B provides a common software platform that serves as a unifying foundation for diverse broadband devices.\\nRDK-B emerges as a comprehensive and forward-thinking solution, offering a unified software platform that not only prioritizes reliability and flexibility but also embraces innovation and compatibility across diverse access networks.\\n \"\\nHow has RDK for Broadband framework been architected to allow for greater innovation? Is it versatile enough for various Broadband technologies?,\"One of the key strengths of RDK-B lies in its open-source architecture, empowering developers with the flexibility to innovate and build upon the software. Its modular design not only facilitates continuous innovation but also optimizes the platform for deployment across a spectrum of devices, ranging from low- to high-end. Notably, RDK-B supports containerization at the application layer, enhancing scalability and flexibility in deploying applications.\\n\\nMoreover, the framework ensures application and services portability across various technologies, promoting a cohesive and integrated user experience. Another significant advantage is the platform\\'s robust support for various access networks, making it a versatile choice for different broadband technologies. RDK-B seamlessly accommodates DOCSIS, PON, DSL, 5G, and Ethernet, ensuring compatibility with a wide array of access technologies. Furthermore, it boasts support for multiple management protocols and data models, contributing to its adaptability and ease of integration.\\n\\nIn essence, RDK-B emerges as a comprehensive and forward-thinking solution, offering a unified software platform that not only prioritizes reliability and flexibility but also embraces innovation and compatibility across diverse access networks.\\n\"\\nWhat are some special highlights about how RDK-B has been constructed? What is so special about its architecture? How do all components within RDK communicate?,\"RDK-B is developed as a modular software stack built from a collection of individually reusable software components and is based on the following design considerations:\\n\\nSoftware modularity\\nAbstraction of external management protocols\\nIndependence from wide area network type\\nSilicon independence\\nLinux kernel independence\\nSoftware structure that allows multiple organizations and teams to work in parallel\\n\\nThe architecture supports pluggable component modules which communicate over the CCSP message bus. RDK-B uses a collection of protocol agent components and supports multiple device management protocols(TR-069, SNMP etc). Protocol agents process the protocol specific details and provide abstraction to the common internal data model.\\n\\nTR-181 data model is the common internal data model used by all RDK-B components to communicate over the message bus. RDK-B also supports multiple SoC vendors through component level hardware abstraction layers.\"\\nHow can I contribute back to the opensource Xmidt components,\"The entire xmidt ecosystem was created by Comcast, and is now opensource and maintained by different maintainers of xmidt-org on Github. Anyone is free to submit features and bug fixes to the project. The process outline is:\\n\\nFork the current open source repository to our own repository.\\nClone the forked repository and do changes.\\nRequest a pull request for the same and await review from the project maintainers.\\nIncorporate the feedback and release more pull requests.\\nMerge of our approved changes to open source repository\"\\nHow can I add tracing capabilites to Xmidt components?,\"The Tracing utility for Xmidt is made available by a component called Candelight. It is available on Github at github.com/xmidt-org/candlelight. \\n\\n\\nXmidt Candlelight has been built using capabilities of open telemetry. \\nImplementation of tracing in various Xmidt microservices, is based on the candlelight module in each of the Xmidt Microservices.\\nCandlelight has been released to open source community for usage.\\nAlso, changes to chrysom, ancla, scytale and tr1d1um repositories have been merged with the master branch of the respective repositories.\\nxmidt-org/candlelight\\nAn OpenTelemetry integration that provides an easy way to add operational visibility to webservices.\\nLanguage: Go\\n\\nThe tracing feature is optional through configuration for the following microservices:\\nTr1d1um\\nscytale\\nArgus\\nCaduceus\\nTalaria\\nPetasos\\nThemis\"\\nTell me everything about Prometheus with respect to Observability,\"here\\'s a breakdown of Prometheus in the context of Observability:\\n\\nUnderstanding Observability through Prometheus\\n\\nWhat is Observability?\\nObservability is the practice of understanding a system\\'s internal state by analyzing the data it produces. This helps assess an application\\'s health and pinpoint issues quickly. It forms the foundation for ensuring that your systems meet Service Level Objectives (SLOs).\\n\\nThe Three Pillars of Observability:\\nObservability relies on three pillars: Metrics, Logs, and Traces.\\n\\nMetrics: Numerical measurements that represent a system\\'s health at a particular point in time. These are collected and stored over time, forming time series data.\\nLogs: Records of events that occur within a system, providing context about what\\'s happening.\\nTraces: Detailed records of the path taken by a single request as it travels through a system, providing insights into how the system functions.\\nPrometheus and Metrics:\\nPrometheus is an open-source system focused on collecting and analyzing metrics. It excels at:\\n\\nMulti-dimensional Data Model: Stores metrics along with timestamps and key-value pairs (labels) for detailed analysis.\\nPromQL: A powerful query language for filtering and analyzing time series data based on labels.\\nEfficient Storage: Leverages a time series database optimized for efficient metric storage and retrieval.\\nAlerting: Defines rules to trigger alerts based on specific metric conditions.\\nVisualization: While not its core function, Prometheus integrates with tools like Grafana for creating informative dashboards.\\nWhy is Prometheus Valuable for Observability?\\n\\nSimplicity: Easy to set up and use, with a focus on collecting metrics through a pull model (Prometheus pulls data from targets).\\nFlexibility: PromQL allows for intricate analysis of metrics.\\nScalability: Can handle large deployments with proper configuration.\\nOpen Source: Freely available and backed by a strong community.\\nPrometheus in Action\\n\\nImagine a web application experiencing slowdowns. Metrics collected by Prometheus might reveal a spike in request processing time. By analyzing these metrics and potentially correlating them with logs (not directly supported by Prometheus), you can pinpoint the root cause, such as a sudden surge in traffic or a database query inefficiency.\"\\nWhat is xmidt in RDK?,\"Xmidt, in the context of RDK (Reference Design Kit), refers to the Management System for WebPA (Web Protocol Adapter).\\n\\nHere\\'s a breakdown of what Xmidt does:\\n\\nWebPA Management: WebPA is a communication protocol that facilitates cloud-based management of RDK devices in home networks. Xmidt acts as the management system for WebPA, likely providing functionalities like:\\nDevice Configuration: Configuring and managing various settings on RDK devices remotely.\\nData Collection and Monitoring: Collecting and analyzing data from RDK devices for monitoring purposes.\\nSoftware Updates: Deploying software updates to RDK devices.\"\\nWhat is RDK-B (Reference Design Kit for Broadband)? Tell me everything you know about it?,\"here\\'s what I know about RDK-B (Reference Design Kit for Broadband):\\n\\nWhat is RDK-B?\\n\\nRDK-B stands for Reference Design Kit for Broadband. It\\'s an open-source software platform designed for use in broadband devices like modems, routers, and gateways.\\n\\nKey Features of RDK-B\\n\\nStandardization:  RDK-B promotes consistency across various broadband devices by providing a common software foundation. This benefits both manufacturers (OEMs) and service providers (NSPs) in several ways:\\n\\nReduced Development Costs: Manufacturers can leverage pre-built components instead of developing everything from scratch.\\nFaster Time to Market: Standardization allows for quicker development cycles.\\nSimplified Management: Service providers have more control and can deploy services efficiently across a wider range of devices.\\nModular Design:  RDK-B consists of modular components that can be customized to specific device functionalities. This flexibility allows for innovation on top of the core platform.\\n\\nOpen Source:  Being open source fosters collaboration and faster development. It also ensures transparency and broadens the pool of developers who can contribute to the platform\\'s improvement.\\n\\nSupported Functionalities:  RDK-B provides a software stack that handles various functionalities in broadband devices, including:\\n\\nHome Networking: Facilitates communication between devices within a home network.\\nWi-Fi Management: Enables configuration and control of Wi-Fi features.\\nDevice Management: Provides tools for remote management and diagnostics of the device itself.\\nInternet of Things (IoT): Supports integration with IoT devices using protocols like Bluetooth, Thread, and Zigbee.\\nBenefits for Users:  Ultimately, RDK-B contributes to a potentially better user experience by enabling:\\n\\nMore Feature-Rich Devices: Manufacturers can leverage RDK-B to create devices with a wider range of functionalities.\\nImproved Performance: Standardization can lead to better optimized and more reliable devices.\\nFaster Service Deployment: Service providers can roll out new features and services more efficiently across RDK-B-based devices.\\nWho Uses RDK-B?\\n\\nNetwork Service Providers (NSPs): Major cable and internet service providers utilize RDK-B to manage their broadband networks and deploy services to their customers.\\nOriginal Equipment Manufacturers (OEMs): Manufacturers of broadband devices leverage RDK-B as a foundation for their products.\\nOverall, RDK-B plays a significant role in standardizing and streamlining the development and deployment of broadband devices, potentially leading to a more efficient and feature-rich user experience.\"\\nWhat is Webpa within RDK-B (Reference Design Kit for Broadband)?,\"Within RDK-B (Reference Design Kit for Broadband), WebPA (Web Protocol Adapter) refers to a communication protocol that facilitates cloud-based management of RDK devices in home networks.\\n\\nHere\\'s a breakdown of what we know about WebPA:\\n\\nWebPA\\'s Role in RDK-B\\n\\nCloud Management: WebPA enables communication between RDK devices and a cloud-based management system. This allows service providers to remotely manage and configure devices, deploy software updates, and monitor their health.\\nStandardized Interface: WebPA provides a standardized interface for communication between RDK devices and the management system. This ensures interoperability between devices from different manufacturers.\"\\nHow do Prometheus metrics work?,\"Prometheus metrics are the foundation for collecting and analyzing data about how a system behaves over time. Here\\'s a breakdown of how they work:\\n\\nThe Prometheus Cycle:\\n\\nExposure:  Applications or systems being monitored need to expose their metrics. This is typically done through:\\n\\nClient Libraries: Libraries provided by Prometheus for various programming languages allow applications to directly publish metrics to Prometheus.\\nExporters: Standalone programs can be deployed to scrape data from systems that don\\'t natively expose metrics and translate it into a Prometheus-compatible format.\\nScraping:  Prometheus periodically scrapes metrics from the configured targets (applications or exporters) using HTTP requests.\\n\\nStorage and Processing:  Scraped metrics are stored locally in Prometheus\\'s time series database.  Prometheus can also perform computations on the data, generating new metrics based on existing ones.\\n\\nAlerting:  Prometheus allows you to define alerting rules based on metric values. If a rule\\'s condition is met (e.g., CPU usage exceeds a threshold), Prometheus triggers alerts and sends notifications.\\n\\nQuerying and Visualization:  Prometheus provides a powerful query language called PromQL for filtering and analyzing time series data based on metric names and labels.  Tools like Grafana can then be used to visualize the queried data in dashboards for easier comprehension.\\n\\nBenefits of Prometheus Metrics:\\n\\nStandardized Approach: Prometheus offers a uniform way to collect and analyze metrics from various sources, simplifying monitoring setups.\\nFlexibility: PromQL enables intricate analysis of metrics based on labels and time ranges.\\nScalability: Prometheus can handle large deployments with proper configuration for scraping and storage.\\nOpen Source: Being open source makes it freely available and fosters a strong community for support and development.\"\\nWhat is Consul? And how does it help service discovery?,\"Consul is a service networking solution developed by HashiCorp. It acts as a central control plane that automates various tasks related to service discovery, service mesh, traffic management, and even automated network updates. Here\\'s a breakdown of how Consul helps with service discovery:\\n\\nUnderstanding Service Discovery:\\n\\nIn modern architectures with microservices, applications are built as a collection of small, independent services. These services communicate with each other to fulfill user requests. Service discovery becomes crucial in this context, as it allows services to dynamically find and connect to other services they need to interact with.\\n\\nConsul as the Registry:\\n\\nConsul acts as a distributed service registry. Here\\'s what that means:\\n\\nService Registration: Services running on different machines or containers can register themselves with Consul. This registration process involves providing information like the service name, IP address, port number, and health checks.\\nDynamic Service Catalog: Consul maintains a constantly updated catalog of all registered services. This catalog acts as a single source of truth for service locations and health.\\nService Discovery for Clients: Applications or services that need to interact with other services can query Consul\\'s service catalog to discover the location (IP address and port) of the desired service. Consul provides various mechanisms for service discovery, including DNS and an HTTP API.\\nBenefits of Consul for Service Discovery:\\n\\nDynamic and Scalable: Consul can handle a large number of services and automatically updates the service catalog as services register, deregister, or change their state.\\nResilience: Consul is designed to be highly available and fault-tolerant. Even if some Consul nodes fail, the service discovery process continues to function.\\nSecurity: Consul offers features for securing service-to-service communication and access control to the service catalog.\\nSimplified Development: By centralizing service discovery, Consul reduces the complexity of developing microservices that need to find and interact with each other.\\nConsul in Action:\\n\\nImagine a microservice architecture where a frontend service needs to communicate with a backend service. Here\\'s how Consul helps:\\n\\nThe backend service registers itself with Consul, providing its details like name, IP, and port.\\nThe frontend service queries Consul\\'s service catalog to discover the location of the backend service.\\nConsul provides the backend service\\'s IP and port, allowing the frontend service to establish communication and make requests.\\n\\nBy leveraging Consul for service discovery and its broader service networking features, you can build more resilient, scalable, and manageable microservice architectures.\"\\nTell me about Caduceus,\"Caduceus is the The XMiDT server for delivering events from talaria to the registered consumer. \\nCaduceus has one function: to deliver events to a consumer. To enable this, caduceus has three endpoints: 1) receive events, 2) register webhooks, and 3) get webhooks.\\n\\nNotify - api/v3/notify endpoint\\nThe notify endpoint will accept a msgpack encoding of a WRP Message. If a webhook is registered and matches the device regex and event regex, the event will be sent to the webhook\\'s registered url. To register a webhook, refer to the webhook registration section\\n\\nWebhook - /hook endpoint\\nTo register a webhook and get events, the consumer must send an http POST request to caduceus that includes the http url for receiving the events and a list of regex filters. The following is an example request. Note: this is not a valid json because of the added comments.\\n\"\\nHow is Device Management done in RDK? ,\"Device Management is an indispensable requirement to support large scale deployment. This helps in:\\n\\nControlled bulk operations/data retrievals\\nFirmware(code) download\\nDynamically enable/disable features\\nRetrieving field matrices\\nXCONF/DCM is a management service that delivers certain configuration details to set-top-boxes (STB). Remote devices like set top boxes have settings to control certain activities. For instance, STBs need to know when to upload log files, or when to check for a new firmware update. In order to remotely manage a large population of devices, we need a solution that lets support staff define instructions and get the instructions to the devices. These are Device Configuration Manager (DCM) and Xconf (X1 Configuration Manager). DCM and Xconf are management services that delivers certain configuration details to set-top-boxes (STB). \\n\\nXconf (X1 Configuration Manager) and Device Configuration Manager (DCM) began as separate solutions. Both the server and STB implementations evolved separately, yet both had overlapping functionality. Now, engineers have consolidated the server modules. The single name of the device configuration management service is XCONF. In the client side, the scripts and components are still named as DCM.\\n\\nDevice Management feature is horizontally scaled between different RDK profiles (like RDK-V, RDK-B and RDK-C) as shown in the below diagram. This means that the same code can be shared between different RDK profiles.\\n\\nXCONF has client component ie CPE device component and server side component. XCONF\\'s primary purpose is to tell STBs where to send their log files and what telemetry metrics they should be sending. XCONF does not receive logs or telemetry from the boxes. It simply tells the CPE where to send log files or telemetry and what specific metrics it should be sending. XCONF also tells CPEs when, where (host), and how (protocol) to send this data.\\n\\nXCONF server consists of two web applications: the Data Service part and Angular-admin (UI). CPEs query the XCONF Data Service to get their configuration details. Angular-admin (UI) allows administrators to create targeting rules and enter all the necessary configuration details for a rule.\\n\\nDevice Management features (using XCONF) include:\\n\\nCode Download (Xconf, DCM)\\nLog Upload\\nTelemetry\\nRDK Feature Control (RFC)\"\\nTell me about WEBPA or Tr1d1um APIs,\"Tr1d1um is an implementation of the WebPA API which enables communication with TR-181 data model devices connected to the XMiDT cloud as well as subscription capabilities to device events.\\n\\nThe WebPA API operations can be divided into the following categories:\\n\\nDevice Statistics - /stat endpoint\\nFetch the statistics (i.e. uptime) for a given device connected to the XMiDT cluster. This endpoint is a simple shadow of its counterpart on the XMiDT API. That is, Tr1d1um simply passes through the incoming request to XMiDT as it comes and returns whatever response XMiDT provided.\\n\\nCRUD operations - /config endpoints\\nTr1d1um validates the incoming request, injects it into the payload of a SimpleRequestResponse WRP message and sends it to XMiDT. It is worth mentioning that Tr1d1um encodes the outgoing WRP message in msgpack as it is the encoding XMiDT ultimately uses to communicate with devices.\\n\\nEvent listener registration - /hook(s) endpoints\\nDevices connected to the XMiDT Cluster generate events (i.e. going offline). The webhooks library used by Tr1d1um leverages AWS SNS to publish these events. These endpoints then allow API users to both setup listeners of desired events and fetch the current list of configured listeners in the system.\"\\nWhat are some important abbreviations in RDK ecosystem: ,\"ACS: Auto Configuration Server\\nADR: Architecture Decision Record\\nANID: Anonymized ID\\nAZ: Availability Zone\\nBB: Broadband\\nBCRP: Binding Corporate Rules Privacy\\nBFF: Back-End For Front-End\\nBNG: Broadband Network Gateway�\\nBNG-EL: Broadband Network Gateway�EasyLogin\\nBSS: Business Support Systems�\\nCA: Certificate Authority\\nCCSP: Common Component Software Platform\\nCDN: Content Delivery Network�\\nCDP: Commissioned Data Processing\\nCDP: Customer Data Platform\\nCDPA: Commissioned Data Processing�Agreement\\nCPE: Customer Premises Equipment (device)\\nCRL: Certificate Revocation List\\nCWMP: CPE WAN Management Protocol (TR-069)\\nDOCSIS: Data Over Cable Service Interface Specification\\nDPS: Digital Platform Service\\nFQDN: Fully Qualified Domain Name\\nJWT: JSON Web Token�\\nMAC Address: Media Access Control\\nMoCA: Multimedia over Coax Alliance\\nMQTT: Message Queuing Telemetry Transport\\nMR: Merge Request, Pull Request\\nMTA: Multimedia Terminal Adapter\\nMTBF: Mean Time Between Failures\\nmTLS: Mutual TLS\\nMTTA: Mean Time To Acknowledge\\nMTTF: Mean time to failure\\nMUS: Master User Story\\nNatCo: National Company\\nNBI: Northbound Interfaces\\nNSP: Network Service�Platform\\nNSP: Network Service Provider\\nNT: Network Technology\\nOCSP: Online Certificate Status Protocol\\nODN: Optical Distribution Network\\nOLT: Optical Line Terminal\\nONT: Optical Network Terminals\\nPAM: Privileged Access Management\\nPGP: Pretty Good Privacy\\nPII: Personal Information\\nPIT, PITR: Point-in-time recovery\\nPON: Passive Optical Network\\nRDK: Reference Design Kit\\nRPO: Recovery Point Objective\\t\\nRTO: Recovery Time Objective\\nSASL: Simple Authentication and Security Layer\\nSID: Shared Information and Data Model\\nSSDP: Simple Service Discovery Protocol\\nSSE: Server-Sent Events\\nSSID: Service Set Identifier\\nTLS: Transport Layer Security\\nWRP: Web Routing Protocol\\nZTP: Zero Touch Provisioning\\n\"\\nTell me about RDK,\"The Reference Design Kit (RDK) is an open-source initiative initiated by Comcast (https://www.comcast.com) \\nRDK provides various \\'profiles\\'. The \\'\\'profile\\' for Home Gateways (CPE, router) is RDK-B\\n\\nThe Reference Design Kit for Broadband (RDK-B) is an open-source initiative standardizing software functionalities for broadband devices enabling MSOs (Multiple System Operator) to efficiently deploy services to a large customer base.\\n\\nThe primary purpose of RDK-B is to create a standardized software stack providing OEM\\'s with specific features to manage complex broadband functions such as\\n\\nWide Area Networking (WAN),\\nLocal Area Networking (LAN),\\ndata reporting and management,\\nhome-networking technologies such as Wi-Fi and Multimedia over Coax Alliance (MoCA) and Internet of Things (IoT) controllers.\\nwiki.rdkcentral.com/display/RDK/RDK+Central+Wiki\\n\"\\n\"What is a \"\"Bounded Context\"\"?\",\"Bounded context is a central pattern in Domain-Driven Design. \\n\\nA bounded context is a boundary within a domain where a particular domain model applies.\\n\\nA domain model acts as a Ubiquitous Language to help communication between software developers and domain experts.\\nIt acts as the conceptual foundation for the design of the software itself - how it\\'s broken down into functions.\\n\\nA model needs to be unified: it needs to be internally consistent so that there are no contradictions within it. \\n\\nA bounded context groups functionality according to whether various functions will share a single domain model.\\n\\nsource: martinfowler.com/bliki/BoundedContext\\nsource: martinfowler.com/bliki/UbiquitousLanguage\"\\nWhat component handles Device Configuration in the RDK Ecosystem?,\"Device Configuration\\nDevice Management for RDK-B based CPE, using Xconf and/or Xmidt\\n\\nWebPA\\n- read/write access to device management TR-181 parameters on a CPE.\\n- Alternative to TR-069.\\n\\nXconf:\\n- Remote Feature Control\\n- Firmware Update\\n- Telemetry Data configuration\\n- Log Data configuration\"\\nWhat component handles Real-Time Device Configuration in RDK Ecosystem?,\"WebPA\\n\\nReal Time Data base on WRP (Web Routing Protocol)\\nread/write access to device management parameters in an efficient manner as compared to TR-69 or SNMP.\"\\nWhat component handles Firmware Image Update in RDK Ecosystem?,\"Xconf Firmware upgrade\\n\\nProvides CPE\\nWhich firmware version\\nFrom where to download\\nHow (protocol) to download\\nXconf web interface for server side rule administration\\nDownload protocol\\nHTTPs as preferred method\\nAbility to decouple downloads from reboot\\nAbility to schedule firmware checks (During boot-up/Later), Configurable based on time zones, quiet times\\nAbility to redirect to secure download endpoints\\nSupports\\nUpgrade of Primary firmware\\nRemote Control\\nDisaster recovery images\\nWarehouse upgrades\\nXconf Client (CPE) Service & Scripts\"\\nWhat component is responsible for VoIP provisioning in RDK Ecosystem?,\"\\t\\nVoIP is supported on the gateway through the analog telephony ports.\\n\\nXmidt (WebPA) is a protocol that is used to expose the internal (TR-181 based) data model to the back-end.\\n\\nThe data model to provision VoIP is TR-104i2.\\n\\nSky: open sourcing the data model abstraction interface to a call manager\\nTechnicolor: deliver the call manager.\\ntemporary solution - RfQ to select a vendor to provide one common voice stack for all RDK-B gateways.\"\\nHow is CPE Telemetry managed?,\"\\t\\nSystem configuration of CPE managed by XConf\\n\\nXconf\\'s primary purpose is to tell the CPE where to send their log files and what telemetry metrics they should be sending.\\nXconf does not receive logs or telemetry from the CPE.\\nIt simply tells the CPE\\nwhere to send log files or telemetry\\nwhat specific metrics it should be sending.\\nwhen, where (host), and how (protocol) to send this data.\"\\nWhat is the WRP or Web Routing Protocol?,\"Web Routing Protocol -  core message envelope used by Xmidt and it\\'s associated services.\\n\\nMechanism for routing\\ncommunication protocols: websocket, http, socket\\nGoal:\\n\\nenable devices to interact with a centralized web router (Xmidt cluster) that routes the messages to/from various services on the other side.\\ninteract with millions of authorized devices in the field in a scalable manor.\\nOSI Layer Stack-Up:\\n\\nWRP - WS - HTTP - TLS/SSL - TCP - IP - Physical\\nWhy does Xmidt need WRP?\\n\\nThe XMiDT service is designed to be able to travel across different connection types (HTTP, Websocket, Nanomsg, etc) and subsequently needs a mechanism to collect all the details needed to deliver, process and respond to a request.\\nWRP MsgPack Encoding\\n\\nWhen a WRP message is send, it is encoded using MsgPack regardless of how the API caller specifies it (MsgPack, JSON or via HTTP headers).\\nThe Xmidt Cloud API Service (skytale) converts the incoming requests into MsgPack and sends it along.\\nThis conversion is not free.\\nIt costs your request time to be converted to MsgPack, so if you\\'re trying to optimize the performance think about emitting WRP as MsgPack directly (like Tr1d1um does).\"\\nWhat is Xconf OSS?,\"Xconf OSS is a management platform for RDK configuration settings:\\n\\nused by engineers and administrators\\nmanage settings (system configuration) on RDK Device centrally.\\nXconf has CPE device component and server-side component.\\n\\nThe Xconf server consists of two components:\\n\\nthe Data Service part\\nXconf Management UI.\\nThe Xconf Server REST API handle the request from Xconf Management UI and modify the data in Cassandra through the Xconf Data Service:\\n\\nFirmware Configurations,\\nCommon data sets (MAC list, Model and Environment),\\ntelemetry profile,\\nDownload location,\\nRemote Feature Control (RFC) and feature that are created, updated or deleted.\\nThe Xconf Data Service is the main component of Xconf-OSS which communicates with the CPE.\\n\\nThe Xconf Dataservice is responsible to fetch the data from the Cassandra database and send it as a response when a CPE makes a request to the Xconf Data Service for the system configuration. \"\\nTell me about Xmidt.,\"Xmidt (pronounced \"\"transmit\"\") is a set of back-end services that provide a communication layer and data path to devices for near real-time data gathering and steering. The Xmidt services are not made available to users. The ingress and southbound interfaces exposed to the internet are for device communication only.\\n\\nXmidt does not implement any business processes but is an enabler for business processes such as provisioning and de-provisioning and device management. Its primary function and use case is to establish and maintain a secure, bidirectional, and stable communication and data path between RDK-B Devices and upstream services.\\n\\nAn RDK-B device connects to the Xmidt cloud via the Xmidt client.\\nCloud-based services can use the Xmidt cloud API to send and receive messages from the devices (Websocket based, bi-directional, asynchronous communication).\\nPush and pull data from the device.\\nWebPA REST API\"\\nWhere can I visualized the metrics scraped by Prometheus?,\"Monitoring and Visualization of Metrics Data that has been scraped by prompethus can be visualed in Grafana.\\n\\nCan be used for Analyzing and visualizing metrics such as system failure rates, connection, CPU, memory, disk and I/O utilization\\n\\nVarious Dashboards for: System & Service Status Health dashboard, RDK Back-End System KPI\\'s\\n\\nSources: Prometheus, Elasticsearch, Jaeger, AWS Cloud Watch\\n\\nAccess Control:\\nUse Grafana built-in user control and authentication mechanisms to restrict and control access to dashboards\\nLink to User Management to Keycloak.\\n\\nAlerting:\\nalert and trigger actions if predefined conditions are met.\\nAlerts: service down (based on health check)\"\\nHow can Grafana be used with Prometheus?,\"Prometheus excels at collecting and storing time series metrics, but to truly understand those metrics, you need a way to visualize them effectively. That\\'s where Grafana comes in.\\n\\nA Match Made in Monitoring Heaven:\\nNative Prometheus Support: Grafana offers out-of-the-box support for Prometheus as a data source. This means seamless integration with minimal configuration.\\nIntuitive Interface: Grafana provides a user-friendly interface for creating dashboards that display metrics in various formats like graphs, heatmaps, and gauges.\\nRich Visualization Options: Grafana offers a wide range of visualization options, allowing you to tailor dashboards to your specific needs. You can create:\\nLine graphs to track trends over time.\\nBar graphs to compare metrics across different categories.\\nHeatmaps to visualize complex data relationships.\\nScatter plots to identify correlations between metrics.\\nGauge displays for quick at-a-glance views of key metrics.\\nPromQL Integration: Grafana allows you to leverage PromQL, Prometheus\\'s powerful query language, directly within dashboards. This lets you filter, aggregate, and transform metrics before visualization.\\nAlerts and Annotations: You can configure Grafana to display alerts triggered by Prometheus rules within dashboards. Annotations allow you to add contextual information to specific points in time on the graph.\\n\\nThe Grafana-Prometheus Workflow:\\nData Source Configuration: Set up Prometheus as a data source in Grafana, specifying the Prometheus server URL and any authentication details.\\nPanel Creation: Start building your dashboard by adding panels, which are the individual visualization elements.\\nMetric Selection: For each panel, choose the Prometheus metric you want to visualize using PromQL queries.\\nCustomization: Customize the panel\\'s appearance and behavior by adjusting options like graph type, time range, and axis labels.\\nDashboard Layout: Arrange the panels on your dashboard to create a clear and informative overview of your system\\'s health.\\n\\nBenefits of using Grafana with Prometheus:\\nActionable Insights: Visualizing metrics helps you identify trends, anomalies, and potential issues within your system, allowing for proactive troubleshooting.\\nImproved Communication: Clear and concise dashboards can facilitate communication between teams by providing a shared view of system health.\\nCustomizable Views: Grafana empowers you to create dashboards tailored to specific user roles or system components, focusing on the most relevant metrics.\\nMonitoring Efficiency: By consolidating visualizations into dashboards, Grafana reduces the need to jump between different tools for monitoring purposes.\\n\\nBeyond Visualization:\\nWhile visualization is its core strength, Grafana offers additional functionalities that complement Prometheus:\\nTeam Collaboration: Share dashboards and annotations with team members for collaborative monitoring.\\nAlerts Integration: Integrate Grafana with alerting tools to receive notifications based on Prometheus alerts.\\nPlugins: Extend Grafana\\'s functionality through a vast ecosystem of plugins for various data sources and integrations.\\n\\nIn Conclusion:\\nThe combination of Prometheus and Grafana is a powerful toolset for monitoring and observability. Prometheus collects and stores the data, while Grafana transforms it into insightful visualizations that empower informed decision-making. If you\\'re working with Prometheus metrics, leveraging Grafana is a natural step to unlock the full potential of your monitoring data.\"\\nWhat is Consul? And how does it help service discovery?,\"Consul is a service networking solution developed by HashiCorp. It acts as a central control plane that automates various tasks related to service discovery, service mesh, traffic management, and even automated network updates. Here\\'s a breakdown of how Consul helps with service discovery:\\n\\nUnderstanding Service Discovery:\\nIn modern architectures with microservices, applications are built as a collection of small, independent services. These services communicate with each other to fulfill user requests. Service discovery becomes crucial in this context, as it allows services to dynamically find and connect to other services they need to interact with.\\n\\nConsul as the Registry\\nConsul acts as a distributed service registry. Here\\'s what that means:\\nService Registration: Services running on different machines or containers can register themselves with Consul. This registration process involves providing information like the service name, IP address, port number, and health checks.\\nDynamic Service Catalog: Consul maintains a constantly updated catalog of all registered services. This catalog acts as a single source of truth for service locations and health.\\nService Discovery for Clients: Applications or services that need to interact with other services can query Consul\\'s service catalog to discover the location (IP address and port) of the desired service. Consul provides various mechanisms for service discovery, including DNS and an HTTP API.\\n\\nBenefits of Consul for Service Discovery:\\nDynamic and Scalable: Consul can handle a large number of services and automatically updates the service catalog as services register, deregister, or change their state.\\nResilience: Consul is designed to be highly available and fault-tolerant. Even if some Consul nodes fail, the service discovery process continues to function.\\nSecurity: Consul offers features for securing service-to-service communication and access control to the service catalog.\\nSimplified Development: By centralizing service discovery, Consul reduces the complexity of developing microservices that need to find and interact with each other.\\n\\nConsul in Action:\\nImagine a microservice architecture where a frontend service needs to communicate with a backend service. Here\\'s how Consul helps:\\nThe backend service registers itself with Consul, providing its details like name, IP, and port.\\nThe frontend service queries Consul\\'s service catalog to discover the location of the backend service.\\nConsul provides the backend service\\'s IP and port, allowing the frontend service to establish communication and make requests.\\n\\nBeyond Service Discovery:\\nWhile service discovery is a core function, Consul offers a broader set of functionalities under the umbrella of service networking. These include:\\nService Mesh: Consul can be used to create a service mesh that facilitates secure, reliable communication between services.\\nTraffic Management: Consul allows for controlling traffic flow between services for functionalities like load balancing and service rollouts.\\nAutomated Network Updates: Consul can automatically configure network infrastructure (firewalls, load balancers) based on service registration, simplifying network management.\\nBy leveraging Consul for service discovery and its broader service networking features, you can build more resilient, scalable, and manageable microservice architectures.\"\\nHow does the Xconf Admin UI work?,\"Application:\\nAngular-Admin module in xconf-oss has all the code related to UI and its functionality. It uses spring framework�s tools such as webmvc, spring-test. RESTful service to define all the web services. It uses maven as a project build tool and jetty plugin to deploy the application in jetty server. The User Interface is designed on AngularJS framework. JUNIT and Mockito for Unit testing and slf4j, log4j for logger. This module has dependency on other modules present in xconf-oss\\n\\nUser Interface :\\n    The Xconf-Angular-Admin(UI) provides the web interface for the user to define firmware configuration for different set top boxes with regards to their Model, Environment and MAC address. There are different features with different functionality for the user to define:\\nCommon:   It lets the user define different types of Model, Environment, MAC Address\\nFirmware:  It lets the user define the firmware configuration for different set top boxes with respect to MAC address, Environment and Model.\\nDCM: It lets the user define the device configuration by managing formulas, Device setting, log upload settings, voice on demand settings, Upload repository and Test page.\\nWhere device setting is to specify set top box when to send swu/stb requirements in addition to reboot.\\nLog upload setting is to specify STB, when and where to upload logs.\\nThere are three levels of logs based on the cron jobs selected\\nTelemetry:  It lets the user to analyze of trends more powerful with lot of metrics. The telemetry section has the permanent profile, targeting rules and Test page.\\n\\nRest Controller:\\n\\n    The rest controllers are responsible to create, delete, retrieve and update datasets from UI into Cassandra database. It is defined under controller in xconf-angular-admin. These controllers handle the request from UI and modify the data in Cassandra through dataservice. All the request are handled by dispatcher servlet and passed to other servlet as defined in web.xml. The logging functionality defined in web.xml as a filter logs every request handled by the controllers.\\n\\n\"\\nHow does Petasos function?,\"Petasos has one function: to redirect incoming requests to the correct talaria. The two types of requests are from a device looking to connect to talaria and from scytale looking to forward a request to a device. In either case, petasos returns an http 307 redirect to the talaria. Petasos determines the correct talaria via service discovery configuration. Currently, petasos can be configured either to dynamically coordinate talarias via Consul (consul option) or be statically configured (fixed option). Refer to cluster configuration for more information.\\n\\nAny URI paths (e.g. /api/v2/device, /api/v2/device/send) will be redirected to the talaria; petasos doesn\\'t parse or validate the path in the request. In order for petasos to complete the request, the X-Webpa-Device-Name header must be included.\\n\\nA helm chart can be used to deploy petasos to kubernetes\\nhelm install xmidt-petasos deploy/helm/petasos\\n\"\\nHow does Themis function?,\"Themis provides a flexible strategy to issue JWT tokens to devices that need to connect to the XMiDT cluster.\\n\\nEndpoints\\nThere are three main endpoints (directly mapped to servers key, issuer and claims in configuration) this service provides:\\n\\nGET /keys/{KID} - PEM format\\nGET /keys/{KID}/key.json - JWK format\\nThis endpoint allows fetching the public portion of the key that themis uses to sign JWT tokens. For example, Talaria can use this endpoint to verify the signature of tokens which devices present when they attempt to connect to XMiDT.\\n\\nConfiguration for this endpoint is required when the issue endpoint is configured and vice versa.\\n\\nGET /issue\\nThis is the main and most compute intensive Themis endpoint as it creates JWT tokens based on configuration.\\n\\nGET /claims\\nConfiguring this endpoint is required if no configuration is provided for the previous two.\\n\\nJWT Claims Configuration\\nClaims can be configured through the token.claims, partnerID and remote configuration elements. The claim values themselves can come from multiple sources.\\n\"\\nWhat are the primary APIs for Consul?,\"GET     /agent/members              // List members\\n \\nGET     /agent/metrics\\nGET     /agent/metrics?format=prometheus\\n \\nGET     /health/node/:node\"\\nWhat the the primary API endpoints for Tr1d1um or WebPA?,\"<FQDN>:<PORT>/api/v2/\\n \\n    GET     /device/{device_id}/stat                // Get device connection statistics\\n    GET     /device/{device_id}/config              // Get multiple parameter values or attributes\\n    PATCH   /device/{device_id}/config              // Set multiple parameter values or attributes\\n    POST    /device/{device_id}/config/{table}      // Add entry to table\\n    PUT     /device/{device_id}/config/{table}      // Replace table data\\n    DELETE  /device/{device_id}/config/{row}        // Delete entry from table\\n \\n    POST    /hook                               // WebPA listener registration for events\\n    GET     /hooks                              // Retrieve the registered WebPA listeners\"\\nWhat the the primary API endpoints for Scytale?,\"<FQDN>:<HEALTH_PORT>\\n    GET     /health                         // Test health\\n \\n<FQDN>:<SERVICE_PORT>/api/v2\\n        /device/{deviceID}/stat             // Device Statistics\\n        /device/send                        // Send WRP to Device\"\\nWhat the primary Xmidt Event API endpoints served via Caduceus?,\"<FQDN>:<METRIC_PORT>\\n    GET     /metrics\\n \\n<FQDN>:<HEALTH_PORT>\\n    GET     /health         // Test health\\n \\n<FQDN>:<SERVICE_PORT>/api/v3\\n    POST    /notify         // Device Events\\n    POST    /hook           // Register a webhook and get events\"\\nWhat the primary Xmidt Device Control API endpoints served via Talaria?,\"<FQDN>:<CONTROL_PORT>/api/v2\\n \\n    GET                 /device/gate                // Device Gate: status of the gate\\n    POST/PUT/PATCH      /device/gate?open=<boolean>   // raises or lowers the gate.\\n    GET                 /device/drain               // Connection drain indicating whether a drain job is active\\n                                                    // progress of the active job if one is running.\\n    POST/PUT/PATCH      /device/drain               // start a drain job.\\n    DELETE              /device/drain               // cancel any running drain job\"\\nWhat the primary Xmidt Device API endpoints served via Talaria?,\"<FQDN>:<HEALTH_PORT>\\n    GET     /health                     // Test health\\n \\n \\n<FQDN>:<SERVICE_PORT>/api/v2\\n \\n    POST    /                           // POST a JSON, MsgPack or HTTP header\\n                                        // encoded WRP message to be sent to the\\n                                        // specified destination in the WRP.\\n    GET     /device/send                // send WRP to device\\n    GET     /device/{deviceID}/stat     // Device statistics\\n \\n    GET     /devices                    // !!! DO NOT USE IN PRODUCTION\\n                                        // !!! VERY EXPENSIVE COMMAND\\n                                        // Get Connected Devices\\n\\nSource: https://github.com/xmidt-org/xmidt/wiki/XMiDT-API\"\\nWhat do the different API response codes for Talaria APIs mean?,\"For the main Talaria API /api/v2/device, following status codes may be returned in different cases:\\n\\n200: Request successful\\n403: Forbidden\\n404: Device Not Found\\n500: The XMiDT cloud had an issue\\n503: A XMiDT cloud component did not respond in the allowed time\\n504: The device did not respond in the allowed time\\n599: If XMiDT was able to transfer the request and response but the device reported a non-2xx status (see WRP.status_code in message or header)\"\\nWhat are the primary Xmidt Redirect APIs for Petasos?,\"<FQDN>:<HEALTH_PORT>\\n    GET     /health                 // Test health\\n \\n<FQDN>:<SERVICE_PORT>/api/v2\\n    POST    /device/send            // Endpoint that Scytale should connect to for fanout\"\\nWhat are the primary Xmidt Autorization APIs for Themis?,\"<FQDN>:<HEALTH_PORT>\\n    GET     /health         // Test health\\n \\n<FQDN>:<SERVER_PORT>/\\n                            // Fetch the public portion of the key that themis uses to sign JWT tokens.\\n \\n                            // E.g. Talaria can use this endpoint to verify the signature of tokens\\n                            // which devices present when they attempt to connect to XMiDT.\\n    GET     /keys/{KID}                         // PEM format\\n    GET     /keys/{KID}/jwk.json                // JWK format\\n \\n                            // Themis endpoint to create JWT tokens based on configuration.\\n    GET     /issue          // provide claims\\n    GET     /claims\\n\\n\\nThere are three main endpoints (directly mapped to servers key, issuer and claims in configuration) this service provides:\\n\\nGET /keys/{KID} - PEM format\\nGET /keys/{KID}/key.json - JWK format\\nThis endpoint allows fetching the public portion of the key that themis uses to sign JWT tokens. For example, Talaria can use this endpoint to verify the signature of tokens which devices present when they attempt to connect to XMiDT.\\n\\nConfiguration for this endpoint is required when the issue endpoint is configured and vice versa.\\n\\nGET /issue\\nThis is the main and most compute intensive Themis endpoint as it creates JWT tokens based on configuration.\\n\\nGET /claims\\nConfiguring this endpoint is required if no configuration is provided for the previous two.\"\\nWhat are the primary APIs for Argus?,\"<FQDN>:<HEALTH_PORT>\\n    GET     /health             // Test health\\n \\n<FQDN>:<METRIC_PORT>\\n    GET     /metrics            // Metrics\\n \\n \\n<FQDN>:<SERVER_PORT>/api/v1\\n \\nPUT     /store/{bucket}         // Persist Data in  the configured database.\\nGET     /store/{bucket}/{id}    // Return unique record identified by bucket\\n                                    and id.\\nGET     /store/{bucket}         // Return all the Records for the bucket.\\nDELETE  /store/{bucket}/{id}    // Delete a record if exists in configured DB\\n    \"\\nExplain the different functions of Argus as well as its main APIs.,\"Argus has one function: interact with a database whether it is internal or external. To enable this, Argus has two endpoints: 1) individual items, and 2) buckets containing items.\\n\\nCreate Individual Item - store/{bucket}/{id} endpoint\\nThis endpoint allows for clients to PUT an object into Argus. The placeholder variables in the path must contain:\\n\\nbucket - The name used to indicate the resource type of which the stored data represents. A plural form of a noun word should be used for stylistic reasons. By default, the following rules will be enforced:\\n\\nBucket names must be between 3 and 63 characters long.\\nBucket names can consist only of lowercase letters, numbers and hyphens (-).\\nBucket names must begin and end with a letter or number.\\nIf you\\'d like to define your own bucket validation format, check out the userInputValidation.bucketFormatRegex configuration option.\\n\\nID - The unique ID within the name space of the containing bucket. It is recommended this value is the resulting value of a SHA256 calculation, using the unique attributes of the object being represented (e.g. SHA256(<common_name>)). This will be used by Argus to determine uniqueness of objects being stored or updated. Argus will not accept any values for this attribute that is not a 64 character hex string containing only 0-9 and a-f.\\n\\nThe body must be in JSON format with the following attributes:\\n\\nID - Required. Must match the ID provided in the URL.\\ndata - Required. RAW JSON to be stored. Opaque to Argus.\\nttl - Optional. Specified in units of seconds. Defaults to the value of the server configuration option itemMaxTTL. If a configuration value is not specified, the value would be a day (~ 24*60^2 seconds). )\\nAn optional header X-Xmidt-Owner can be sent to associate the object with an owner. The value of this header will be bound to the new item, which would require the same value passed in a X-Xmidt-Owner header for subsequent reads or modifications. This in effect creates a secret attribute bound to the life of newly created items. When provided, Argus validates the length of the owner string to be in the range [10,60]. If you\\'d like to define your own validation format, check out the userInputValidation.ownerFormatRegex configuration option.\\n\\nWhen the header is not provided, the owner of the item will be the empty string.\\n\\nThe exception to the above would be an authorized request. The authorization method is not specified and is up to the implementation to decide. Authorized requests shall be allowed to update all attributes except the X-Xmidt-Owner meta attribute.\\n\\nList - store/{bucket} endpoint\\nThis endpoint allows for GET to retrieve all the items in the bucket organized by the id.\\n\\nAn example response will look like the below where \"\"7e8c5f378b4addbaebc70897c4478cca06009e3e360208ebd073dbee4b3774e7\"\" is the id of the only item in this collection. An optional header X-Xmidt-Owner can be sent with the request. If supplied, only items with secrets matching the supplied value will be returned in the list. If not supplied, all items created without an owner (owner value = \"\"\"\") will be returned. For authorized requests, if no owner header is provided, all items for the specified bucket will be returned.\\n\\nThis endpoint allows for GET, and DELETE REST methods to interact with any object that was created with the previous PUT request. An optional header X-Xmidt-Owner can be sent with the request. All requests are validated by comparing the secret stored with the requested record with the value sent in the X-Xmidt-Owner header. If the header is missing, the \"\"\"\" (empty string) is assigned as the item\\'s owner during item creation. A mismatch will result in a \"\"403 Forbidden\"\" error. An authorized request may override this requirement, providing an administrative override. The method of authorization is not specified.\\n\"\\nGive a short description about WEBPA.,\"WebPA -> Protocol Agent (on the CPE)\\n\\nWebPA is the communication channel from Xmidt to RDK based home gateway devices. WebPA is the implementation of the WebPA API which enables communication with TR-181 data model devices connected to Xmidt as well as subscription capabilities to device events. \"\\nWhat configuration values can I tweak for the JWT Validator in Themis?,\"1.\\tjwtValidator.keys.factory.uri\\t\\nNone\\nThemis endpoint for key\\n2.\\tjwtValidator.keys.purpose\\tNone\\thttps://github.com/xmidt-org/bascule/blob/main/key/purpose.go\\n0 ? Sign (require public key only)\\n1 ? Verify (requires private key only)\\n2 ? Encrypt (requires  private key only)\\n3 ? Decrypt (requires public key only )\\n3.\\tjwtValidator.keys.updateInterval\\tNone\\tWhen to refresh the key from Themis\"\\nDescribe the main function for Talaria as well as its APIs,\"Talaria has two main functions: Device Interactions, and Control Devices:\\n\\nDevice Interaction: Talaria\\'s primary function is to interact with the devices. The communication with the device happens over a websocket using WRP Messages.\\n\\nTalaria can send events originating from the device as well as emit its own events. When this occurs, the event is sent to Caduceus.\\n\\nTalaria has three API endpoints to interact with the devices connected to itself. A XMiDT cloud client should not directly query against a talaria. Instead, they should send a request through scytale.\\n\\nDevice Statistics - /device/{deviceID}/stat endpoint\\nThis will return the statistics of the connected device, including information such as uptime and bytes sent. This request does not communicate with the device, instead the request returns stored statistics.\\n\\nGet Devices - /devices endpoint\\nThis will return a list of all the actively connected devices and their statistics, just like the stat command.\\nWarning: this is an expensive request. Use with caution. This is not recommended to be used in production.\\n\\nSend WRP to Device - /device/send endpoint\\nThis will send a WRP message to the device. Talaria will accept a WRP message encoded in a valid WRP representation - generally msgpack or json. If the message is json encoded, talaria will encode the payload as msgpack. Talaria will then forward the message to the device. If the device returns a message, it will be encoded as the HTTP accept header. msgpack is the default encoding of the wrp message.\\n\\n\\nControl Devices\\nA secondary function of talaria is to control the connected devices. This allows for the flow of devices to go towards specific talarias. In other words, where the websockets are made can be controlled. For more information refer to Control Server Docs.\\n\\nGate Devices - /device/gate endpoint\\nThis will allow or deny devices to connect to the talaria instance.\\n\\nDrain Devices - /device/drain endpoint\\nThis will remove the connected devices from the talaria instance.\\n\\n\"\\nExplain the TR-181 Device Data Model.,\"Technical Report 181 (TR-181)  defines version 2 of the TR-069 Device data model (Device:2).\\n\\nThe Device:2 data model applies to all types of TR-069-enabled devices, including End Devices, Residential Gateways, and other Network Infrastructure Devices.\\n\\nIt is a set of data objects covering things like\\n\\nbasic device information,\\ntime-of-day configuration,\\nnetwork interface and protocol stack configuration,\\nrouting and bridging management,\\nthroughput statistics, and\\ndiagnostic tests.\\nThe cornerstone of the Device Data Model is the interface stacking mechanism. Network interfaces and protocol layers are modeled as independent data objects that can be stacked, one on top of the other, into whatever configuration a device might support.\\n\\nSource:\\n- https://github.com/BroadbandForum/cwmp-data-models\\n- https://device-data-model.broadband-forum.org/\"\\nDescribe the TR-069 CPE WAN Management Protocol,\"TR-069 CPE WAN Management Protocol (CWMP)\\n\\nTechnical Report 069 (TR-069) is a technical specification of the Broadband Forum that defines an application layer protocol for remote management of customer-premises equipment (CPE) connected to an Internet Protocol (IP) network,\\n\\nTR-069 is a protocol for data exchange between the Auto Configuration Server (ACS) of a communication provider and an associated terminal at the customer.\\n\\nA typical use case is the remote configuration of DSL routers by a broadband provider. Technically speaking, it is a bidirectional SOAP protocol for HTTP-based communication between customer premises equipment (CPEs) and auto-configuration servers (ACS). It is therefore also known as CPE WAN Management Protocol (CWMP).\\n\\nsource: \\n- https://cwmp-data-models.broadband-forum.org/\\n- https://en.wikipedia.org/wiki/TR-069\"\\nExplain the main API endpoints for Scytale,\"Scytale has two API endpoints to interact with the devices: 1) get the statistics for a device and 2) send a WRP Message to the device. If the device isn\\'t connected, a 404 is returned.\\n\\nDevice Statistics - /api/v2/device/{deviceID}/stat endpoint\\nThis will return the statistics of the connected device, including information such as uptime and bytes sent. This information is retrieved from the talaria that the device is connected to.\\n\\nSend WRP to Device - /api/v2/device/send endpoint\\nThis will send a WRP message to the device. Scytale will accept a WRP message encoded in a valid WRP representation - generally msgpack or json and will forward the request to the correct talaria.\"\\nWhat are some main features and advantages of RDK or RDK-B?,\"The Reference Design Kit for Broadband(RDK-B) is a standard open source software stack which sits below the applications/services layer and provides a common interface to SoC\\'s acting as a universal SoC adapter that allows portability across hardware platforms.  The RDK-B stack provides complex broadband and management functions such as Home Networking, WiFi, and Device Management. Even though almost all present RDK-B deployments are over DOCSIS gateways, its modular design makes RDK-B an ideal choice for a host of different devices like WiFi extenders, voice gateways or IoT platforms. \\n\\nRDK-B comes with some of the added advantages due to a highly modular architecture which allows the developers to have the provisions for dynamic service discovery, common data model, multiple management interfaces like SNMP, TR069, WebUI. Some of the added advantages of RDK-B are :\\n\\nEasy Integartion \\nPortable\\nExtensible\\nSecure\\nReduced Cost of Maintenance\\nWAN types\\nRDK-B features can be classified broadly into  the following three categories:\\n\\nRDK-B Device Management\\nBroadband Services\\n\\nsource: https://wiki.rdkcentral.com/display/RDK/RDK-B+Features\"\\nWhat is an RDK Based Broadband Router? Explain its advantages as well as its impact on market.,\"The RDK Broadband Routers\\n\\nBroadband routers are essential pieces of hardware for providing internet access in homes and offices. They allow multiple devices to connect to the internet and each other, and provide a variety of features such as network security and traffic management. RDK (Reference Design Kit) is an open-source software platform that has been developed to provide a standardized platform for the development of broadband routers. This article will discuss RDK-based broadband routers, their advantages, and their potential impact on the market.\\n\\nWhat is RDK?\\n\\nRDK is a standardized software platform for the development of broadband routers. It was created by Comcast in 2012 and has since become an open-source project managed by RDK Management LLC. The platform is based on a Linux kernel and provides a set of APIs (Application Programming Interfaces) and services that allow developers to create customized firmware for broadband routers. RDK also includes various components such as drivers, utilities, and middleware that are needed for the operation of the router.\\n\\nAdvantages of RDK-based broadband routers\\n\\nStandardization: RDK provides a standardized platform for the development of broadband routers. This means that different manufacturers can use the same platform and create interoperable devices. This reduces the fragmentation of the market and provides a more consistent user experience.\\nCustomization: RDK provides a set of APIs and services that allow developers to create customized firmware for broadband routers. This means that manufacturers can add their features and functionality to the router, and differentiate their product from competitors.\\nOpen-source: RDK is an open-source platform, which means that it is freely available for anyone to use and modify. This allows manufacturers to reduce their development costs and create more affordable products.\\nSecurity: RDK includes various security features such as encryption, firewall, and user authentication. This helps to protect users from cyber threats and provides a more secure network.\\nScalability: RDK is designed to be scalable, which means that it can be used for both small and large deployments. This makes it a suitable platform for both homes and businesses.\\nCloud integration: RDK supports cloud integration, which means that manufacturers can add cloud-based services to their routers. This allows for remote management, monitoring of the router, and the ability to add new features and functionality through over-the-air updates.\\nImpact on the market\\n\\nRDK-based broadband routers have the potential to have a significant impact on the market. The standardization provided by RDK could lead to a reduction in the number of different platforms and increase interoperability between devices. This could result in a more consistent user experience and reduce the complexity of managing different devices.\\n\\nThe customization provided by RDK could lead to increased competition between manufacturers. Manufacturers could differentiate their products by adding their features and functionality to the router. This could result in a wider range of products with more advanced features, and potentially lower prices as manufacturers can reduce their development costs.\\n\\nThe open-source nature of RDK could also lead to more innovation and collaboration between manufacturers. Developers can contribute to the platform and share their code with others, which could lead to faster development times and a more comprehensive range of features.\\n\\nThe security features provided by RDK could also have a significant impact on the market. As cyber threats become more advanced, users are becoming more concerned about the security of their networks. RDK provides a set of security features that can help to protect users from these threats. This could increase demand for RDK-based routers, as users seek more secure and reliable devices.\\n\\nThe DSL support, GOPN, DOCSIS, and Fixed Wireless Access with 4G and 5G, makes RDK routers even more preferred router backend ecosystem. With the 5G rollout becoming more popular and the increasing cost of fiber layout, broadband routers on the move are preferred by the younger generation.\\n\\nFinally, the cloud integration provided by RDK could lead to a more connected and integrated network. Manufacturers can add cloud-based services to their routers, which can help to manage and monitor the router remotely. All leading telcos offering broadband services are adopting RDK for their customer-premise equipment (CPEs).\\n\\nDeutsche Telekom is actively involved in driving this ecosystem forward and deployment of RDK-based routers across NatCOs within the next few months.\\n\\nsource: https://mauryaravindra.medium.com/what-is-rdk-based-broadband-router-d3ebcf7cd8b6\"\\nWhat is upcoming in Broadband? What does the future of broadband look like?,\"The internet has become an essential part of both the B2B and B2C markets. Businesses rely on broadband networks and routers to communicate with their employees, customers, and partners, while consumers rely on them for entertainment, shopping, and socializing. As technology continues to evolve, the next generation of innovations in broadband networks and routers will be critical to meet the growing demands of both markets.\\n\\nSoftware-Defined Networking (SDN):\\n\\nSoftware-Defined Networking (SDN) is a next-generation innovation that separates the network control plane from the data plane, making it more flexible and easier to manage. With SDN, businesses can dynamically allocate network resources, adjust network policies, and respond to changing business needs in real time. This technology can also improve network security by providing granular control over network access.\\n\\nFor the B2C market, SDN can provide improved network performance and reliability, allowing users to stream videos, play online games, and browse the internet without interruption.\\n\\nNetwork Function Virtualization (NFV):\\n\\nNetwork Function Virtualization (NFV) is another next-generation innovation that enables the virtualization of network functions such as firewalls, routers, and load balancers. With NFV, businesses can deploy network functions as software instead of hardware, reducing costs and increasing agility. This technology can also improve network scalability, making it easier to support large-scale deployments.\\n\\nFor the B2C market, NFV can provide improved network performance and reliability, allowing users to access websites, stream videos, and download files faster and more reliably.\\n\\nWi-Fi 6/6e/7:\\n\\nWi-Fi 6/6e/7 are the latest generation of wireless networking technologies that offer faster speeds, higher capacity, and improved performance. With Wi-Fi 6/6e/7, businesses can support more devices simultaneously, making it easier to connect all their employees and customers. This technology can also improve network efficiency, reducing latency and improving battery life.\\n\\nFor the B2C market, Wi-Fi 6/6e/7 can provide faster speeds and better coverage, allowing users to stream videos, play online games, and browse the internet without interruption.\\n\\nMesh Networks:\\n\\nMesh networks are another next-generation innovation in broadband networks and routers that enable multiple routers to work together to provide seamless coverage throughout a building or area. With mesh networks, businesses can extend their network coverage without the need for additional wiring or infrastructure. This technology can also improve network reliability, as it can automatically re-route traffic in case of a network failure.\\n\\nFor the B2C market, mesh networks can provide better coverage throughout the home, ensuring that users can access the internet from any room.\\n\\nCloud-Managed Networking:\\n\\nCloud-managed networking is a next-generation innovation that enables businesses to manage their networks from a centralized location in the cloud. With cloud-managed networking, businesses can monitor network performance, deploy network updates, and manage network security from anywhere, without the need for on-site IT personnel. This technology can also improve network scalability, making it easier to support large-scale deployments.\\n\\nFor the B2C market, cloud-managed networking can provide improved network performance and reliability, allowing users to access the internet from anywhere in the home without interruption.\\n\\nOpen Source Router Firmware:\\n\\nWith the increasing popularity of open-source software-based router firmware like RDK-B, Tecos have more control over what goes into the router and how it can be managed. With agile deliveries, firmware updates are possible more frequently than they happened in past, based on OEM firmware. This leads to faster delivery of digital experiences for end customers.\\n\\nDigital Experience for Customers\\n\\nBroadband is moving from a commodity to becoming an experience for end users, integrating smart features like security, custom rules, advanced home network management features, integrated shopping, in-app entertainment, self-care, and many more. This changes customer engagement from a Telco App to Digital Experience App in hand.\\n\\nDeutsche Telekom is working on all these highlighted points to bring a next-generation digital experience for its customers across the globe.\\n\\nIn conclusion, next-generation innovations in broadband networks and routers are set to revolutionize both the B2B and B2C markets. From software-defined networking and network function virtualization to Wi-Fi 6, mesh networks, and cloud-managed networking, these innovations promise to deliver faster speeds, higher capacity, improved performance, and better network reliability. As businesses and consumers continue to rely on the internet for communication, entertainment, and shopping, these innovations will be essential to meet the growing demands of the market.\\n\\nsource: https://mauryaravindra.medium.com/whats-upcoming-in-broadband-ce5ac30ea52e\"\\nHow do REST APIs work?,\"REST APIs (Representational State Transfer APIs) are a prevalent method for applications to communicate with each other over the internet. They follow a client-server architecture where data is exchanged through HTTP requests and responses.\\n\\nThe Players Involved:\\n- Client: The application initiating the interaction. It could be a mobile app, a web browser, or another server.\\n- Server: The application that stores and manages the resources accessed through the API. It listens for incoming requests and sends back responses.\\n\\n\\nCore Components:\\n- FQDN (Fully Qualified Domain Name): This is the unique web address of the server hosting the API, like \"\"[invalid URL removed]\"\".\\n- Port: Think of a port as a specific doorway on the server. REST APIs typically use port 80 for regular HTTP traffic or port 443 for secure HTTPS traffic.\\n- Endpoint: This is a specific URL path within the API that identifies the resource the client wants to access. For instance, \"\"/users\"\" might be an endpoint to access user data.\\n\\nThe Request-Response Flow:\\n\\n1. The Client Makes a Request: The client initiates the interaction by sending an HTTP request to the server. This request includes several parts:\\nHTTP Request Type: This verb specifies the desired action on the resource. Common types include:\\nGET: To retrieve data from the server.\\nPOST: To create a new resource on the server.\\nPUT: To update an existing resource.\\nDELETE: To delete a resource.\\nEndpoint: The specific URL path of the resource the client wants to access (e.g., \"\"/users/123\"\" to access user with ID 123).\\nHeaders: Optional information sent along with the request, like authentication details or data format preferences.\\nBody (for POST/PUT): For requests that create or update data, the client might send a body containing the actual data in a format like JSON.\\n\\n\\n2. The Server Sends a Response: Upon receiving the request, the server processes it and sends an HTTP response back to the client. The response contains:\\nStatus Code: A three-digit code indicating the outcome of the request. Common codes include:\\n200 OK: The request was successful.\\n400 Bad Request: The request was invalid due to errors in the data or syntax.\\n404 Not Found: The requested resource could not be found.\\n500 Internal Server Error: The server encountered an error while processing the request.\\nHeaders: Optional information sent along with the response, such as content type or authentication required for future requests.\\nBody (for successful requests): For successful GET or PUT requests, the server might include a body containing the requested data or confirmation of the update.\\n\\n\\n3. The Client Takes Action: Based on the status code and response body, the client application takes appropriate action. For instance, on a successful data retrieval (GET request with status code 200), the client might use the received data to populate its user interface.\\n\\nBenefits of REST APIs:\\n\\nStandardized: REST adheres to widely adopted HTTP protocols, making it easy for different applications to interact.\\nScalable: REST APIs can handle a large number of requests efficiently.\\nFlexible: REST supports various data formats (JSON, XML) and HTTP methods for different actions.\\nStateless: Each request-response interaction is independent, simplifying server-side development.\\nIn essence, REST APIs provide a well-defined way for applications to communicate and exchange data over the web using familiar HTTP concepts.\\n\\nsource: climbtheladder.com/http-protocol-interview-questions/\"\\nExplain everything about the TR-069 protocol in detail,\"TR-069: Remote Management for Customer Premises Equipment\\n\\nTR-069, also known as CPE WAN Management Protocol (CWMP), is a technical specification developed by the Broadband Forum (BBF) that defines an application layer protocol for remote management and provisioning of Customer Premises Equipment (CPE).\\n\\nWhat does TR-069 do?\\n\\nAuto-configuration: CPE devices (like modems, routers, gateways) can be automatically configured by a remote server upon initial setup or after a reset.\\nSoftware/Firmware Management: Service providers can remotely deploy software updates and firmware upgrades to CPE devices.\\nDiagnostics and Monitoring: TR-069 allows for remote retrieval of diagnostic data and monitoring of CPE device performance.\\nStatus and Performance Management: Provides real-time insights into the health and performance of CPE devices.\\nKey Players in TR-069:\\n\\nCPE (Customer Premises Equipment): The device being managed remotely, typically a modem, router, or gateway in a home network.\\nACS (Auto-Configuration Server): A server operated by the service provider that manages and configures CPE devices remotely.\\nCommunication Flow:\\n\\nCPE Initialization: The CPE device initiates communication with the ACS upon startup or after a reset.\\nDevice Discovery: The ACS identifies the CPE device and retrieves its configuration information.\\nConfiguration Download: The ACS sends the CPE device its configuration settings.\\nManagement and Monitoring: The ACS can remotely manage the CPE device by sending commands for tasks like software updates, diagnostics, and performance monitoring.\\nData Transfer: The CPE device can send data reports and diagnostic information back to the ACS.\\nBenefits of TR-069:\\n\\nReduced Costs: Automates device configuration and management, saving time and resources for service providers.\\nImproved Service Delivery: Enables faster troubleshooting, quicker software updates, and better overall service quality.\\nStandardized Management: Provides a common framework for managing CPE devices from various manufacturers.\\nSecurity Considerations:\\n\\nSecure Communication: TR-069 relies on secure protocols like HTTPS and authentication mechanisms to protect communication between the CPE and ACS.\\nPotential Vulnerabilities: Security vulnerabilities in the implementation of TR-069 on a CPE device or ACS could leave the system open to unauthorized access.\\nOverall, TR-069 plays a crucial role in simplifying remote management and provisioning of CPE devices for broadband service providers.\"\\nExplain everything about the TR-181 device data model in detail,\"TR-181, also known as the Device Data Model for CWMP Endpoints and USP Agents, is a technical report by the Broadband Forum (BBF) that defines a standardized data model for exchanging information between Customer Premises Equipment (CPE) devices and management systems. It works hand-in-hand with the TR-069 protocol (CPE WAN Management Protocol) for remote management of CPE devices.\\n\\nUnderstanding the TR-181 Data Model:\\n\\nStandardized Data Representation: TR-181 defines a hierarchical data structure with objects and parameters that represent various aspects of a CPE device\\'s configuration and operational state. This standardization allows different management systems to understand and interpret data received from various CPE devices.\\nFocus on Manageability: The data model primarily focuses on parameters relevant to remote management tasks like configuration, diagnostics, and monitoring.\\nHierarchical Structure: The data model is organized hierarchically, with the \"\"Device\"\" object at the root. Subsequent levels represent specific functionalities of the device, like \"\"InternetGatewayDevice\"\", \"\"LAN\"\", \"\"Wi-Fi\"\", etc. Within each object, there are specific parameters that provide detailed configuration options and operational data.\\nBenefits of TR-181:\\n\\nSimplified Management: The standardized data model enables different management systems to communicate with various CPE devices without needing vendor-specific implementations.\\nInteroperability: TR-181 fosters interoperability between CPE devices from different manufacturers and management systems from various vendors.\\nEfficient Data Exchange: The defined data structure ensures efficient exchange of management information between the CPE and management system.\\nThings to Consider:\\n\\nFocus on Management: While TR-181 covers a wide range of parameters for configuration and monitoring, it doesn\\'t encompass all aspects of a device\\'s functionality.\\nVersioning: The TR-181 data model has gone through several revisions, with newer versions incorporating additional functionalities for managing newer technologies. It\\'s important to ensure compatibility between the CPE device\\'s TR-181 version and the management system\\'s capabilities.\\nBeyond TR-181:\\n\\nData Exposure: CPE devices implement TR-181 by exposing their data model through an interface (typically a web interface) that the management system can access using protocols like HTTP.\\nData Transfer: The TR-069 protocol governs the communication flow between the CPE device and the management system, including the exchange of data based on the TR-181 data model.\\nIn conclusion, TR-181 plays a vital role in enabling standardized and interoperable remote management of CPE devices. By defining a common data model, it simplifies communication between management systems and various CPE devices, leading to more efficient and streamlined network management for service providers.\"\\nWhat databases are used in OneShop\\'s inventory management system?,MongoDB for operational data and Elastic Search for real-time search capabilities on the front-end.\\nWhat role does Netflix Conductor play in the OneShop inventory management system?,Netflix Conductor is used as an orchestration engine to manage workflows between microservices for tasks such as calculating capacity status and updating catalog status.\\nHow is real-time synchronization achieved between sales view and warehouse stock in OneShop?,\"Real-time synchronization is achieved through event-driven architecture, where MongoDB change events trigger workflows in Netflix Conductor to update the system status.\"\\nWhat is the purpose of the Mongo Change Stream Event Listener in the inventory management system?,It listens to changes in MongoDB and triggers the Conductor workflows to process these changes and update the system status accordingly.\\nHow does the inventory management in OneShop system handle device reservation?,\"When an item is added to a shopping cart, it is reserved for a set duration (X minutes), during which the inventory count is temporarily adjusted.\"\\nWhat happens when a device reservation expires in OneShop\\'s system?,\"If the order is not placed within the reservation time, the system automatically reverts the inventory status and updates the catalog to reflect the availability.\"\\nDescribe the process when a new device batch is received in the inventory service that is implemented in OneShop.,\"An admin updates the stock count in the inventory system, which triggers a synchronization process to reflect these changes across the sales channels.\"\\nHow are stock levels updated in real-time on the OneShop website and app?,\"Elastic Search documents are updated via workflows managed by Netflix Conductor to reflect the latest inventory status, ensuring that the most current data is displayed.\"\\nWhat ensures the IN-STOCK status of devices in the sales catalog Service in OneShop?,\"The IN-STOCK status is maintained based on real-time data from MongoDB, processed through event-driven workflows to update Elastic Search documents\"\\nWhat happens when a device is set for PRE-ORDER in the inventory system?,\"Devices available for PRE-ORDER are marked accordingly in the system and displayed on sales channels, with stock availability managed separately until actual stock is received.\"\\nHow does OneShop\\'s inventory management system handle OUT-OF-STOCK status?,\"The OUT-OF-STOCK status is automatically set when inventory levels drop to zero, with updates propagated to sales channels through event-driven updates.\"\\nExplain the flow of a device status update from IN-STOCK to OUT-OF-STOCK in inventory Service of OneShop.,\"When the last unit of a device is reserved, the inventory status changes to OUT-OF-STOCK, triggered by the change stream and processed by the event-driven core.\"\\nHow are data loads into the sales inventory initiated?,\"Data loads can be initiated manually through an admin UI or automatically via integration with warehouse systems, which then trigger status synchronization processes.\"\\nWhat microservices architecture is utilized in the OneShop inventory management?,\"The system uses Spring Boot-based microservices for different functionalities like sales inventory, sales catalog, and more, interconnected through event-driven workflows.\"\\nHow are changes in the sales inventory reflected in the sales catalog?,Changes in inventory trigger MongoDB events that are processed by Conductor workflows to update device status in the sales catalog maintained in Elastic Search.\\nDescribe how reservation expiration impacts device stock status in inventory service in OneShop Product,\"When a reservation expires without a purchase, the stock status is updated back to available, triggering synchronization across the inventory and sales catalog.\"\\nWhat mechanisms are in place to ensure inventory data consistency across OneShop\\'s platform?,Event-driven workflows ensure consistency by updating all relevant components in real time as inventory changes occur.\\nHow does the system prevent overbooking of devices that are low in stock in context of inventory Service in OneShop?,The system reserves devices only for a limited duration and updates inventory in real time to prevent overbooking based on current stock levels.\\nWhat happens if there is a system failure during a stock status update?,\"The system is designed to handle failures gracefully, ensuring data consistency and reliability through robust error handling and recovery mechanisms within the microservices architecture.\"\\nHow is the user informed about the availability of devices on OneShop\\'s platform?,\"Device availability is communicated through real-time updates on the website and app, directly reflecting the current stock status from the inventory system.\"\\nWhat happens during a transition of device state from IN-STOCK to OUT-OF-STOCK?,\"This transition occurs when a device\\'s last unit is reserved or sold, triggering event-driven updates to mark the device as OUT-OF-STOCK across all channels.\"\\n\"Can you explain how OneShop\\'s sales inventory solution manages real-time synchronization across different channels, handles device states and reservations, updates inventory dynamically, and integrates various technological components like MongoDB, Elastic Search, and Netflix Conductor to maintain accurate and timely inventory updates, while addressing challenges such as multi-tenancy, synchronization between physical inventory and device pool databases, and ensuring the consistency of device state information across sales channels?\",\"The sales inventory solution of OneShop, our e-commerce product, is a multitenant solution unified for all channels including OneShop, OneApp, etc., and natcos. It ensures near-real-time synchronization between sales views and warehouse stock. The inventory management flow involves an admin updating stocks of devices in the inventory whenever a new batch of devices is received. A user on a channel such as web or app shows interest in devices available in stock, and as soon as the user adds a device to his shopping cart, it gets reserved for a specified number of minutes. If a user places an order within this timeframe, the device count is subtracted from the stock. Once devices from the pool are exhausted, the admin updates the pool again. The system handles a few important states of products: IN-STOCK if more than one unit of a device is available in the warehouse, OUT-OF-STOCK if no units are available, and available for PRE-ORDER if it is a new exclusive launch. The synchronization challenge involves ensuring that the device listing powered from the Sales Catalog reflects the correct state (IN-STOCK, OUT-OF-STOCK, PRE-ORDER) of the device. Creating a reservation in Sales Inventory for the specified minutes may update the status of the device if there was only one unit of the device left in stock, signifying a transition of device state from IN-STOCK to OUT-OF-STOCK. Cancelling a reservation in Sales Inventory after the specified minutes should influence the status of the device if there was only one unit of the device left in stock, signifying a transition of device state from OUT-OF-STOCK to IN-STOCK. We addressed the synchronization between physical inventory and devices pool database, how to reserve a device for a specified duration, ensuring the device reservation expires at the correct time, achieving consistency between real-time user orders and inventory updates by an admin, and updating Elastic Search documents in the sales catalog with the correct state to ensure IN-STOCK devices come on top. The tech stack includes MongoDB, Elastic Search, Netflix Conductor (orchestration engine), and Spring Boot-based microservices. We broke down the problem into four parts: Status synchronization event-driven core is used as the center of the implementation to synchronize the status of the device due to operations like inventory load, reservation updates, and orders. Any update operation in sales inventory (POST/PUT) leads to a change in data in MongoDB, generating a change event which is listened by a change stream event listener running in the sales inventory microservice. This listener starts a conductor workflow which then starts a task calculate_capacity_status executed in an inventory microservice to compute the status of the device by reading available stock and count of the number of reservations at the moment. The conductor then makes a decision whether there is a change of status in the sales catalog, and if the state change is applicable, it starts a task update_catalog_capacity_status to update the state of the device in ES. This task is executed in the sales catalog microservice to update the status of a device from IN-STOCK to OUT-OF-STOCK and vice versa. Load data into sales inventory can be done manually by admin or real-time requests from the warehouse through inventory admin UI or automatically through a distributor connected to the warehouse. POST API operations on sales inventory lead to the data change in MongoDB which then executes the \\'Status synchronization event-driven core\\' shown above. Sales inventory updates on add to cart involve an addition of an item to a shopping cart or submitting an order which can lead to a change in the state of the device. Whenever an item is added to a shopping cart, it is reserved for the specified duration in a temporary document with a TTL within the reservation collection. When the reservation expires, documents get deleted from the collection and change stream events trigger the \\'Status synchronization event-driven core\\'. The same happens in the case of submitting an order, meaning promotion of reservation to order, hence the actual count of inventory is decremented and triggers the \\'Status synchronization event-driven core\\'. Exposure of status to channels involves querying the status or count of each device left in stock as Sales inventory exposes simple GET operations from its core for this purpose. The solution highlights include a modular and event-driven design, with multiple microservices being loosely coupled, the use of mongo change event to listen to every change in the state of inventory, conductor workflows to remove coupling between sales catalog and sales inventory, and a single driver to update the change in the state of devices.\"\\nWhat led OneShop to implement CICD?,Inefficiencies in building microservices directly in production environments prompted the need for CI/CD.\\nHow did manual processes affect deployment at OneShop before CICD?,\"Manual processes made deployments time-consuming and error-prone, requiring multiple steps and checks.\"\\nWhat was OneShop\\'s approach to branching before adopting CICD?,\"OneShop followed a release-based branching approach, creating branches per release cycle.\"\\nWhat challenges did OneShop face with release-based branching?,\"This method caused delays in integration, made builds complex, and CI setup confusing.\"\\nWhat are the benefits of environment-based branching adopted by OneShop?,\"It supports CI more effectively, allows multiple release versions, and provides clarity in deployment.\"\\nHow does the dev branch function in OneShop\\'s development lifecycle?,\"It is used for active development, where developers commit ongoing changes and enhancements.\"\\nWhat is the role of the QA/Intermediate branch in OneShop?,\"It handles advanced testing and integration, ensuring features work seamlessly with existing code.\"\\nDescribe the transition from QA to release branches in OneShop.,\"After QA confirms stability, code moves to the Release branch for final adjustments before production.\"\\nWhat is the purpose of the Master branch in OneShop?,\"It represents the most stable version of the code, ready for production deployment.\"\\nHow does back-merging support software development in OneShop?,\"It ensures all branches are updated with the latest tested code, reducing conflicts.\"\\nExplain the function of multiple development environments in OneShop.,\"They allow isolated testing by developers, minimizing disruptions in the main development flow.\"\\nWhat role do staging environments play at OneShop?,They replicate production conditions for performance testing and final adjustments.\\nHow does the Preprod environment contribute to OneShop\\'s deployment process?,It\\'s used for final verifications by QA teams before the software goes live.\\nWhat checks are performed in the UAT environment at OneShop?,User Acceptance Testing to ensure the software meets all user expectations.\\nWhat types of tests are conducted in the CI/CD pipeline at OneShop?,\"Unit tests, integration tests, and performance tests to ensure code quality and functionality.\"\\nHow do Nexus profiles facilitate OneShop\\'s development process?,\"They help manage artifacts specific to each development stage, ensuring quick access and consistency.\"\\nWhat automation tools are integrated in OneShop\\'s CI/CD pipeline?,\"Jenkins for automation of builds and deployments, GitLab for repository management.\"\\nHow does OneShop handle version control for its software development?,\"Through GitLab, where all code is version-controlled, enabling easy tracking and management of changes.\"\\nWhat is the significance of artifact management in OneShop\\'s CI/CD?,Artifact management ensures reliable and consistent access to necessary resources during builds and deployments.\\nWhat strategies are used for managing older releases at OneShop?,\"Older releases are managed through tags and separate branches, allowing for ongoing support and bug fixes.\"\\n\"How has the implementation of Continuous Integration and Continuous Deployment, CICD  transformed the software development lifecycle at OneShop, particularly in addressing the challenges of manual deployments across multiple micro-services? Could you discuss the specific strategies employed, such as the adoption of environment-based branching, the roles of different branches (\\'dev\\', \\'qa\\', \\'release\\', and \\'master\\'), and the integration of Nexus profiles for managing artifacts? Additionally, how have these changes facilitated a smoother workflow and reduced the time to market for new releases?\",\"A development process involves crafting code and initiating deployment, a practice that unfortunately resulted in significant time consumption for OneShop. The reason was that micro-services were built directly in the environments without knowing if the code was in a buildable state. Like many of us, we\\'d fix things as we went along, but it was clear that we needed to streamline this process. That\\'s when Continuous Integration and Continuous Deployment (CI/CD) entered the scene. Before we dive into the problem, let�s take a quick stroll down memory lane to see how things used to be: We followed the Agile methodology, working in two-week sprints and branching our code based on release patterns like release-. (e.g., release-23.20). Publishing shared JAR files on Nexus was a manual task, which, as you can imagine, could get quite tedious. The deployment process required multiple clicks and checks just to ensure a single deployment went smoothly. And to top it all off, we were doing manual deployments for 10�15 micro-services. Can you say �time-consuming�? So yeah, things needed a bit of a makeover. When you see �manual,� it�s a sign of trouble. And those slow deployments caused by the assembly line of jar building? We need to make that process faster and smoother! The Branching Strategy The brainstorming session started by taking a hard look at the current way of managing branches. This involved keeping a branch for each release cycle, using one version for all shared libraries, and doing manual builds (with Jenkins, but still a hands-on process) It had its perks; it made keeping track of versions and supporting older releases straightforward. But, as you can imagine, it had its fair share of problems too like delayed integration, a complicated build process, and a confusing CI setup. In the next step, we explored different branching strategies like trunk-based, feature-based, forking, and environment-based. After lots of discussions and brainstorming, the environment-based branching made the most sense. This environment-based branching strategy came with some real perks. For starters, it made Continuous Integration (CI) much easier to handle. It could still support multiple release versions using tags, and having a separate branch for each environment gave more control and clarity in the deployment process. So, all in all, it was a win-win. To tackle existing branching & manual deployment issues, a structured approach was adopted by introducing four specific branch types: �dev,� �qa,� �release,� and �master.� For a clearer understanding, please review the diagram below. Software development lifecycle in OneShop, incorporating branching strategies, deployment environments, and configuration management via Nexus profiles. It begins with the \\'Dev\\' branch, a primary arena for active development where developers commit ongoing changes and enhancements. From here, changes are merged into the \\'QA/Intermediate\\' branch for advanced testing and integration, ensuring that new features integrate seamlessly with existing code. This approach mitigates risks and promotes stable builds. Once the QA process confirms the build\\'s stability, the code progresses to the \\'Release\\' branch. This branch serves as a pre-production holding area, where final adjustments are made before the software is deemed ready for release. Post-validation in the release branch, the code is merged into the \\'Master\\' branch, which represents the codebase\\'s most stable version, ready for deployment to the production environment. Throughout this process, back-merging plays a critical role in ensuring all branches remain updated with the latest tested and approved code, minimizing conflicts and discrepancies across the development lifecycle. Specifically, code is frequently merged back from the \\'QA/Intermediate\\' branch to the \\'Dev\\' branch and from the \\'Release\\' branch back to both \\'QA/Intermediate\\' and \\'Dev\\'. This ensures that any fixes or updates integrated during the testing phases are reflected back in the development branches. The environments section of the diagram delineates several stages of deployment: multiple development environments (\\'Dev\\' and \\'Dev2\\') allow developers to test in isolated settings, preventing disruptions in the main development flow. Several staging environments (\\'Stage2\\', \\'Stage3\\', \\'Stage4\\', \\'Stage5\\' \\'Stage6\\', \\'Stage7\\' and a general \\'Stage\\') replicate the production environment, which is crucial for performance testing and final tweaks. The \\'Preprod\\' and \\'UAT\\' (User Acceptance Testing) environments are pivotal for final verifications by quality assurance teams and clients respectively, ensuring the software meets all user expectations and requirements. The \\'Prod\\' environment is where the fully tested and approved application is finally deployed, accessible to end-users. Additionally, the \\'Mkt\\' environment could suggest a specialized setting used for marketing or demonstrations to stakeholders. Lastly, the Nexus profiles outlined (\\'eshop-dev\\', \\'eshop-qa\\', \\'eshop-release\\', \\'eshop-prod\\', and \\'eshop-stage\\') correspond to different stages of the development pipeline in a Nexus repository\"\\n\"What led OneShop to evaluate its performance under heavy load conditions, and what were the initial findings?\",\"OneShop evaluated its performance under heavy load to prepare for high-demand scenarios like sales or festive seasons. The initial findings revealed that the system did not scale up as expected, leading to failures and errors, particularly in device and tariff listing pages.\"\\nWhat specific issues did OneShop face with its Sales Catalog APIs during high traffic periods?,\"During high traffic periods, the Sales Catalog APIs were unable to handle the sudden increase in load, resulting in slow response times and errors, which directly impacted user experience and sales performance.\"\\nHow did OneShop first attempt to solve the scalability problems with its APIs?,\"OneShop\\'s initial approach to solving scalability problems involved horizontal scaling, specifically by increasing the number of Sales Catalog API Pods to distribute the load more effectively.\"\\nDescribe the process and outcome of increasing the number of Sales Catalog API Pods at OneShop.,\"By increasing the number of Sales Catalog API Pods, OneShop was able to enhance the throughput of the system, allowing it to handle more requests per unit of time. However, this solution reached a limit, and failures started occurring again once that limit was exceeded.\"\\nWhat changes did OneShop make to its ElasticSearch Cluster to improve performance?,\"OneShop increased the number of nodes and the size of each node in its ElasticSearch Cluster. This expansion helped in better managing the data and requests, which significantly reduced API failures.\"\\nHow did reducing the response size initially impact OneShop\\'s API performance?,\"Reducing the response size by removing unnecessary HTML content from the API responses was intended to improve performance. Although this change did result in some improvement, it was not as significant as expected, suggesting other underlying performance issues.\"\\n\"Why did OneShop decide to introduce a caching layer, and what was expected?\",\"OneShop introduced a caching layer to achieve faster response times for duplicate requests, aiming to reduce the load on the backend servers and improve overall system performance.\"\\nWhat limitations did OneShop encounter after introducing caching to its system?,\"Despite introducing caching, the performance improvements plateaued around 600 requests per second (RPS), which was already achievable through previous scaling efforts. This indicated that caching alone was insufficient to handle peak loads effectively.\"\\nHow did thread dump analysis help OneShop identify performance bottlenecks?,\"Thread dump analysis revealed that application threads were frequently in a WAITING state due to synchronous log writing, identifying excessive logging as a major performance bottleneck.\"\\nWhat specific actions did OneShop take to optimize its logging practices?,\"OneShop optimized its logging by removing unnecessary logs, reducing log size, and moving to asynchronous log writing, which significantly relieved the load on the system by reducing waiting times for log writes.\"\\nHow did asynchronous log writing improve OneShop\\'s system performance?,\"Asynchronous log writing minimized the waiting time of application threads that was previously caused by synchronous log operations, thereby enhancing the overall efficiency and responsiveness of the system.\"\\nWhat further code optimization did OneShop implement after the second round of thread dump analysis?,\"After identifying additional inefficiencies in the code, OneShop corrected unnecessary object-string conversions and streamlined various processes to further reduce system latency and improve API response times.\"\\nHow did network limitations of EC2 machines affect OneShop\\'s API scalability?,\"Network limitations on EC2 machines restricted the response size per second, which limited the system\\'s ability to scale up during peak traffic periods. OneShop considered increasing instance sizes and using burst credits to temporarily enhance capacity.\"\\nWhat role did burst credits play in OneShop\\'s performance strategy?,\"Burst credits allowed OneShop\\'s EC2 instances to temporarily exceed their baseline performance limits during unexpected spikes in traffic, providing a short-term solution to maintain service levels.\"\\nHow did the state of the AWS Elastic File System (EFS) impact OneShop\\'s performance during high traffic periods?,\"The state of the AWS Elastic File System (EFS), particularly when it became filled, significantly reduced the system\\'s throughput. This highlighted the importance of monitoring and managing EFS capacity to avoid performance degradation.\"\\nWhat metrics did OneShop focus on improving through their various optimizations?,\"OneShop focused on improving throughput, measured in requests per second (RPS), and response times, as reflected in their P95 graphs, which indicated significant enhancements from 600 RPS to around 2800 RPS after optimizations.\"\\nHow did OneShop utilize the ELK Stack in their performance optimization efforts?,\"The ELK Stack was used extensively by OneShop for logging and monitoring. It helped in analyzing log data and performance metrics, which were crucial for identifying issues and measuring the impact of implemented changes.\"\\nDescribe the role of Grafana and Prometheus in OneShop\\'s monitoring strategy.,Grafana and Prometheus were used by OneShop to fetch critical metrics and visualize performance data. These tools provided real-time insights into system health and were instrumental in proactive performance management.\\nHow did AWS CloudWatch contribute to OneShop\\'s ability to manage its infrastructure?,\"AWS CloudWatch provided OneShop with first-hand information on infrastructure metrics, allowing them to monitor system performance, set alarms, and react swiftly to potential issu\"\\n\"What ongoing challenges does OneShop face in maintaining API performance, and what is their approach moving forward?\",\"OneShop recognizes that maintaining API performance is an ongoing challenge, requiring continual monitoring, testing, and optimization. They plan to keep refining their strategies and adapting to new technologies to ensure scalability and reliability.\"\\n\"Provide me the blog link which was written by Pratish Bhatnagar from OneShop Backend on scaling up SLC API, or Sales Catalog API.\",Here is the detailed blog on SLC API scaling which was one : https://medium.com/deutsche-telekom-gurgaon/how-we-scaled-up-the-sales-catalog-api-19eb582b5c97\\n\"Can you provide a detailed overview of the scalability challenges faced with the Sales Catalog API at OneShop, the strategies implemented for SLC API scaling, and the overall impact on system performance, including the specific measures taken to optimize API response times and manage system load effectively?\",\"Scaling Sales Catalog APIs and, in turn, scaling OneShop APIs. The backstory goes like this�a few months back, the business decided to check how OneShop performs when put under heavy load and in a sudden burst of traffic. To our surprise, the system did not scale up as expected and instead started failing and throwing errors to the end user. Our device and tariff listing pages had failed us. What did we do? As any engineer would when faced with API failures due to load, the first thing we did was to increase the number of Sales Catalog API Pods (Horizontal Scaling). This increased the throughput to an extent, but we started getting failures after a point. The next thing that came to our mind was to increase the resources available to the APIs. In this case, it meant increasing the number of nodes in our ElasticSearch Cluster and also increasing the size of each ElasticSearch Node in the cluster. This further reduced the number of API failures and provided us with much-needed time to dig deeper into this problem. We started analyzing the response from each layer, the databases, and each API being called to serve the listing response. We found out that we were passing HUGE pieces of HTML Content (Thank you, Admins :-) ) that were of no use in the listing response. So, we chalked that out. And voila! The APIs started performing as expected! Sadly, this was not the case. The performance improved but was nowhere near what we wanted. We introduced a separate caching layer in our application because why not! The expectation after introducing caching was clear: blazing-fast responses for duplicate requests. This is what we thought as well. Yet again, this was not the case. In our load tests, we found that the throughput did not increase much even after adding a caching layer. It started plateauing around 600 RPS. This was what we had already achieved after scaling up the production environment and was not something we were aspiring for.We tried taking thread dumps from pods and started analyzing them. We found something very unexpected during our analysis. Until this point, we had not expected our logs to be a bottleneck for the application. After all, these are the best weapons we have when fighting a production issue. During our thread dump analysis, we found that the application threads were in a WAITING state just to write logs. This opened multiple fronts for us: we are printing too many unnecessary logs, the log size is very large and needs to be reduced, since our response payload can be fairly large, it meant an increased load on the logging system, and the logs are written in a synchronous fashion. We performed these steps to overcome these problems: we removed the unnecessary logs and updated the log levels as per the severity, we stopped the unnecessary printing of objects in logs, we removed the response objects from our logs, and we moved to asynchronous writing of logs. After fixing the logs, we went again to further analyze the thread dumps. We found further issues with the code. The threads were still in a WAITING state. We found that we were unnecessarily converting objects to string and vice versa before responding to the request. We fixed this as well and were ready for our last leg. After all the optimizations, we started performing regular load tests on the APIs and overnight soak tests. We observed that after multiple successful runs, an odd run would fail unexpectedly, or the system would not scale as much, or would scale down and plateau to a constant RPS. Upon digging deep into this problem, we read about machine network limitations and burst credits. To summarize, each EC2 machine can only send out a limited response size per second. This can be increased by increasing the instance size or momentarily using burst credits. Now we had found the answers! We had achieved the limits of the available hardware and were even using the burst credits. This was the reason for the dip in response and plateauing of the RPS graph. In our tests, we found that the throughput depended upon the state of the AWS Elastic File System as well. This is the place where we write our logs. When the EFS filled up, the RPS reduced and the system would not scale up. We use the ELK Stack for our logging needs. So, we leveraged that to plot graphs to measure the response times against the critical operation times. These should be very much near each other. We also used Grafana and Prometheus to fetch the critical metrics of the application. We also used AWS CloudWatch extensively to get first-hand information on our infrastructure. Using the above-mentioned approach, we were able to increase the throughput of the APIs from 600 RPS to around 2800RPS. Our P95 Graphs also improved indicating the improvement in response times. We were able to increase these metrics and the system survives under spikes and huge loads. However, the API performance track cannot be closed and is a never-ending process.\"\\n', metadata={'source': './data/data.txt'})]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders =  [\n",
    "    TextLoader(\"./data/data.txt\")\n",
    "]\n",
    "docs = []\n",
    "for l in loaders:\n",
    "    docs.extend(l.load())\n",
    "len(docs)\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving larger chunks\n",
    "Sometimes, the full documents can be too big to want to retrieve them as is. In that case, what we really want to do is to first split the raw documents into larger chunks, and then split it into smaller chunks. We then index the smaller chunks, but on retrieval we retrieve the larger chunks (but still not the full documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents - The big chunks\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "\n",
    "# This text splitter is used to create the child documents - The small chunks\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=bge_embeddings)\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_chunks_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_chunks_retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"what is scytale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give a trivia or a fun fact about Scytale. How do I pronounce the word Scytale? ,\"Scytale is actually the stick for transposition cipher. In Ancient Greek, it means a 'baton' or a 'cylinder'. It is pronounced as \"\"skit-uh-lee\"\".\"\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = big_chunks_retriever.invoke(\"what is scytale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1727"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Data is visualized by Grafana (Prometheus and Elasticsearch as data sources).\"\n",
      "Give me some trivia and fun facts about Xmidt.,Xmidt cloud and associated components are named after Greek gods or items of the Greek gods\n",
      "Tell me a fun fact about Parodus. Is Parodus a Greek word?,\"Parodus is actually an ancient greek word for \"\"entrance\"\".\"\n",
      "Tell me a Fun fact about Talaria. Is it related to any Greek God in any way?,\"Talaria were actually the winged sandals, a symbol of the Greek messenger god Hermes. It is pronounced as \"\"tuh-laa-ree-uh\"\".\"\n",
      "Give a trivia or a fun fact about Scytale. How do I pronounce the word Scytale? ,\"Scytale is actually the stick for transposition cipher. In Ancient Greek, it means a 'baton' or a 'cylinder'. It is pronounced as \"\"skit-uh-lee\"\".\"\n",
      "\"How does one pronounce \"\"Petasos\"\"? Also, tell me something interesting about Petasos.\",\"Petasos in Greek legends was actually the funky hat worn by the Greek messenger god Hermes, and is pronounced as \"\"pet-uh-sos\"\".\"\n",
      "What is the origin of the word Caduceus? Give me a trivia about the word along with its pronounciation. ,\"Caduceus in Greek legends was the name for Hermes� wand. It is said the wand would wake the sleeping and send the awake to sleep. It was symbol of commerce and negotiation. Pronounced as \"\"kuh-doo-see-us\"\".\"\n",
      "\"Who was Themis? How is the Themis component in the Xmidt ecossytem related to Greek mythology? Tell me something interesting about Themis,\",\"Themis was an ancient Greek Titaness. She is described as deity of good counsel, order and righteousness.\"\n",
      "What Signing algorithm does Themis use for its Tokens?,Themis used the alogorithm RSA-256 for signing. RSA-256 is the required JWT signing algorithm to use by Themis.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mretrieved_docs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = Ollama(model=\"brutus-no-sys\"),\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=big_chunks_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Talaria is a critical service within Xmidt Cloud that enables communication between clients (devices/routers/CPEs) and the backend. Its primary function is to manage millions of device connections, forward device events, and send/receive requests from devices using secure WebSocket connections. Additionally, Talaria utilizes WRP messages for efficient communication within Xmidt Cloud. It acts as a gatekeeper for device access to Xmidt Cloud services through JWT (JSON Web Token) authorization.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is talaria do?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
